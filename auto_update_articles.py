import os
import random
import datetime
import re
import json
import shutil
import urllib.parse
import sys
import subprocess
import hashlib
import platform
from PIL import Image, ImageDraw, ImageFont

# è‡ªåŠ¨å®‰è£…æ‰€éœ€çš„ä¾èµ–
try:
    from PIL import Image
    print("PILå·²æˆåŠŸå¯¼å…¥")
except ImportError:
    print("æ­£åœ¨å®‰è£…PIL/Pillowåº“...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "Pillow"])
    from PIL import Image
    print("PIL/Pillowå®‰è£…å’Œå¯¼å…¥æˆåŠŸ")

from pathlib import Path
from update_sitemap import update_sitemap  # å¯¼å…¥sitemapæ›´æ–°åŠŸèƒ½

# é…ç½®
ARTICLES_DIR = 'articles'
ARTICLES_CONFIG = 'articles_config.json'
LOG_FILE = 'update_log.txt'
BACKUP_DIR = 'articles_backup'  # æ–‡ç« å¤‡ä»½ç›®å½•
IMAGES_DIR = 'images'  # å›¾ç‰‡ç›®å½•
MAX_IMAGE_WIDTH = 1200  # æœ€å¤§å›¾ç‰‡å®½åº¦
IMAGE_QUALITY = 85  # å›¾ç‰‡å‹ç¼©è´¨é‡

# æ£€æµ‹æ“ä½œç³»ç»Ÿç±»å‹
IS_WINDOWS = platform.system() == 'Windows'

# æ·»åŠ å†…å®¹åŒºå—æ ‡è¯†ç¬¦
CONTENT_BLOCK_MARKERS = {
    'latest_update': '<!-- AUTO_UPDATE_BLOCK: LATEST_UPDATE -->',
    'latest_update_end': '<!-- AUTO_UPDATE_BLOCK_END: LATEST_UPDATE -->',
    'new_insight': '<!-- AUTO_UPDATE_BLOCK: NEW_INSIGHT -->',
    'new_insight_end': '<!-- AUTO_UPDATE_BLOCK_END: NEW_INSIGHT -->',
    'related_articles': '<!-- AUTO_UPDATE_BLOCK: RELATED_ARTICLES -->',
    'related_articles_end': '<!-- AUTO_UPDATE_BLOCK_END: RELATED_ARTICLES -->',
    'schema_markup': '<!-- AUTO_UPDATE_BLOCK: SCHEMA_MARKUP -->',
    'schema_markup_end': '<!-- AUTO_UPDATE_BLOCK_END: SCHEMA_MARKUP -->',
    'social_meta': '<!-- AUTO_UPDATE_BLOCK: SOCIAL_META -->',
    'social_meta_end': '<!-- AUTO_UPDATE_BLOCK_END: SOCIAL_META -->',
    'mobile_style': '<!-- AUTO_UPDATE_BLOCK: MOBILE_STYLE -->',
    'mobile_style_end': '<!-- AUTO_UPDATE_BLOCK_END: MOBILE_STYLE -->'
}

def log_message(message):
    """è®°å½•æ—¥å¿—"""
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f'[{timestamp}] {message}\n')
    print(f'[{timestamp}] {message}')

def generate_content_id(content_type, article_path):
    """ç”Ÿæˆå†…å®¹åŒºå—çš„å”¯ä¸€IDï¼Œç”¨äºè·Ÿè¸ªæ›´æ–°"""
    filename = os.path.basename(article_path)
    current_date = datetime.datetime.now().strftime('%Y%m%d')
    unique_string = f"{content_type}_{filename}_{current_date}"
    return hashlib.md5(unique_string.encode()).hexdigest()[:8]

def clean_marked_blocks(content, block_type):
    """æ¸…ç†å…·æœ‰æ ‡è®°çš„å†…å®¹åŒºå—"""
    start_marker = CONTENT_BLOCK_MARKERS[block_type]
    end_marker = CONTENT_BLOCK_MARKERS[f"{block_type}_end"]
    
    # å°è¯•æŸ¥æ‰¾å¹¶åˆ é™¤æ ‡è®°çš„åŒºå—
    pattern = re.escape(start_marker) + r'[\s\S]*?' + re.escape(end_marker)
    if re.search(pattern, content):
        cleaned_content = re.sub(pattern, '', content)
        return cleaned_content, True
    
    return content, False

def has_marked_block(content, block_type):
    """æ£€æŸ¥å†…å®¹ä¸­æ˜¯å¦å·²å­˜åœ¨æŒ‡å®šç±»å‹çš„æ ‡è®°åŒºå—"""
    start_marker = CONTENT_BLOCK_MARKERS[block_type]
    return start_marker in content

def load_config():
    """åŠ è½½æ–‡ç« é…ç½®"""
    if os.path.exists(ARTICLES_CONFIG):
        with open(ARTICLES_CONFIG, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        # é»˜è®¤é…ç½®
        default_config = {
            "articles": [
                {"file": "seo-guide.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "responsive-design.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "ui-ux-design.html", "update_frequency": 2, "last_updated": "", "type": "core", "keywords": []},
                {"file": "website-development.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "mobile-app-development.html", "update_frequency": 2, "last_updated": "", "type": "core", "keywords": []},
                {"file": "ecommerce-solutions.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "cloud-services.html", "update_frequency": 2, "last_updated": "", "type": "core", "keywords": []},
                {"file": "conversion-rate.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []}
            ]
        }
        with open(ARTICLES_CONFIG, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=4)
        return default_config

def save_config(config):
    """ä¿å­˜æ–‡ç« é…ç½®"""
    with open(ARTICLES_CONFIG, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=4)

def should_update_article(article_config):
    """åˆ¤æ–­æ–‡ç« æ˜¯å¦éœ€è¦æ›´æ–°ï¼Œæ ¹æ®æ–‡ç« ç±»å‹è°ƒæ•´æ›´æ–°é¢‘ç‡"""
    if not article_config.get('last_updated'):
        return True
    
    last_updated = datetime.datetime.strptime(article_config['last_updated'], '%Y-%m-%d')
    days_since_update = (datetime.datetime.now() - last_updated).days
    
    # æ ¹æ®æ–‡ç« ç±»å‹è°ƒæ•´æ›´æ–°é¢‘ç‡
    article_type = article_config.get('type', 'data')
    base_frequency = article_config.get('update_frequency', 7)
    
    # æ ¸å¿ƒå†…å®¹æ›´æ–°é¢‘ç‡é™ä½ï¼Œæ•°æ®å†…å®¹æ›´æ–°é¢‘ç‡æé«˜
    if article_type == 'core':
        return days_since_update >= base_frequency * 1.5  # æ ¸å¿ƒå†…å®¹æ›´æ–°é¢‘ç‡é™ä½
    else:
        return days_since_update >= base_frequency  # æ•°æ®å†…å®¹æ­£å¸¸æ›´æ–°

def backup_article(article_path):
    """å¤‡ä»½æ–‡ç« """
    # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
    filename = os.path.basename(article_path)
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_filename = f"{os.path.splitext(filename)[0]}_{timestamp}{os.path.splitext(filename)[1]}"
    backup_path = normalize_path(os.path.join(BACKUP_DIR, backup_filename))
    
    # å¤åˆ¶æ–‡ä»¶
    shutil.copy2(article_path, backup_path)
    log_message(f"å·²å¤‡ä»½æ–‡ç« : {filename} -> {backup_filename}")
    return backup_path

def extract_keywords(content):
    """ä»æ–‡ç« å†…å®¹ä¸­æå–å…³é”®è¯"""
    # æå–æ‰€æœ‰h2, h3æ ‡ç­¾å†…å®¹å’ŒåŠ ç²—æ–‡æœ¬ä½œä¸ºå…³é”®è¯
    keywords = []
    
    # æå–æ ‡é¢˜
    h2_matches = re.findall(r'<h2>(.*?)</h2>', content)
    h3_matches = re.findall(r'<h3>(.*?)</h3>', content)
    
    # æå–åŠ ç²—æ–‡æœ¬
    bold_matches = re.findall(r'<strong>(.*?)</strong>', content)
    
    # æå–é•¿å°¾å…³é”®è¯ï¼ˆæ®µè½ä¸­çš„å…³é”®çŸ­è¯­ï¼‰
    p_content = ' '.join(re.findall(r'<p>(.*?)</p>', content))
    p_content = re.sub(r'<.*?>', '', p_content)  # ç§»é™¤HTMLæ ‡ç­¾
    
    # ç®€å•çš„é•¿å°¾å…³é”®è¯æå–ï¼ˆ3-5ä¸ªè¯çš„çŸ­è¯­ï¼‰
    words = p_content.split()
    for i in range(len(words) - 2):
        if i + 4 < len(words):  # 4è¯çŸ­è¯­
            phrase = ' '.join(words[i:i+4])
            if len(phrase) > 10 and not any(char.isdigit() for char in phrase):  # ç®€å•è¿‡æ»¤
                keywords.append(phrase)
    
    # åˆå¹¶å…³é”®è¯
    keywords.extend(h2_matches)
    keywords.extend(h3_matches)
    keywords.extend(bold_matches)
    
    # æ¸…ç†å…³é”®è¯
    cleaned_keywords = []
    for kw in keywords:
        # ç§»é™¤HTMLæ ‡ç­¾
        kw = re.sub(r'<.*?>', '', kw)
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        kw = re.sub(r'[^\w\s]', '', kw)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        kw = kw.strip()
        if kw and len(kw) > 1:  # åªä¿ç•™æœ‰æ„ä¹‰çš„å…³é”®è¯
            cleaned_keywords.append(kw)
    
    return list(set(cleaned_keywords))  # å»é‡

def update_article_date(article_path):
    """æ›´æ–°æ–‡ç« çš„å‘å¸ƒæ—¥æœŸ"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ›´æ–°æ–‡ç« æ—¥æœŸ
    today = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    new_content = re.sub(
        r'<span class="article-date"><i class="far fa-calendar-alt"></i>\s*\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥</span>',
        f'<span class="article-date"><i class="far fa-calendar-alt"></i> {today}</span>',
        content
    )
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def add_latest_update_section(article_path, article_config):
    """æ·»åŠ æœ€æ–°æ›´æ–°åŒºå—"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æŸ¥æ‰¾æ–‡ç« å†…å®¹å¼€å§‹ä½ç½®ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€ä¸ªh1æˆ–h2æ ‡ç­¾ä¹‹åï¼‰
    article_start = None
    h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', content)
    h2_match = re.search(r'<h2[^>]*>(.*?)</h2>', content)
    
    if h1_match:
        article_start = h1_match.end()
    elif h2_match:
        article_start = h2_match.end()
    
    if article_start is None:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {os.path.basename(article_path)} ä¸­æ‰¾åˆ°æ–‡ç« å¼€å§‹æ ‡è®°")
        return False
    
    # æ¸…ç†æ—§çš„æœ€è¿‘æ›´æ–°åŒºå—
    original_size = len(content)
    cleaned_content = content
    bytes_removed = 0
    cleaned_count = 0
    
    # æ¸…ç†å®Œæ•´çš„æœ€è¿‘æ›´æ–°åŒºå—ï¼ˆåŒ…å«å¼€å§‹å’Œç»“æŸæ ‡è®°ï¼‰
    log_message(f"å¼€å§‹æ¸…ç†æ–‡ç«  {os.path.basename(article_path)} ä¸­çš„æ—§æœ€è¿‘æ›´æ–°åŒºå—")
    pattern = re.escape(CONTENT_BLOCK_MARKERS['latest_update']) + r'[\s\S]*?' + re.escape(CONTENT_BLOCK_MARKERS['latest_update_end'])
    if re.search(pattern, cleaned_content):
        cleaned_content = re.sub(pattern, '', cleaned_content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {os.path.basename(article_path)} ä¸­çš„å®Œæ•´æœ€è¿‘æ›´æ–°åŒºå—")
        cleaned_count += 1
    
    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„æœ€è¿‘æ›´æ–°åŒºå—æ¡†
    pattern = r'<div class="latest-update-box">[\s\S]*?</div>\s*</div>'
    if re.search(pattern, cleaned_content):
        cleaned_content = re.sub(pattern, '', cleaned_content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {os.path.basename(article_path)} ä¸­çš„æœ€è¿‘æ›´æ–°åŒºå—æ¡†")
        cleaned_count += 1
    
    # æ¸…ç†å¯èƒ½æ®‹ç•™çš„æ ·å¼
    pattern = r'<style>\s*\.latest-update-box[\s\S]*?</style>'
    if re.search(pattern, cleaned_content):
        cleaned_content = re.sub(pattern, '', cleaned_content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {os.path.basename(article_path)} ä¸­çš„æœ€è¿‘æ›´æ–°æ ·å¼")
        cleaned_count += 1
    
    # æ¸…ç†å…¶ä»–å¯èƒ½çš„æœ€è¿‘æ›´æ–°åŒºå—å˜ä½“
    patterns = [
        r'<div[^>]*class="[^"]*update[^"]*"[^>]*>[\s\S]*?</div>\s*</div>',
        r'<section[^>]*class="[^"]*update[^"]*"[^>]*>[\s\S]*?</section>',
        r'<div[^>]*id="[^"]*update[^"]*"[^>]*>[\s\S]*?</div>\s*</div>'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, cleaned_content)
        if matches:
            cleaned_content = re.sub(pattern, '', cleaned_content)
            cleaned_count += len(matches)
    
    if cleaned_count > 0:
        log_message(f"å·²æ¸…ç†æ–‡ç«  {os.path.basename(article_path)} ä¸­çš„é¢å¤–æœ€è¿‘æ›´æ–°åŒºå—ï¼Œå…± {cleaned_count} ä¸ª")
    
    bytes_removed = original_size - len(cleaned_content)
    
    if bytes_removed > 0:
        log_message(f"æ–‡ç«  {os.path.basename(article_path)} ä¸­å…±æ¸…ç†äº† {bytes_removed} å­—èŠ‚çš„æ—§æœ€è¿‘æ›´æ–°å†…å®¹")
    else:
        log_message(f"æ–‡ç«  {os.path.basename(article_path)} ä¸­æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ—§æœ€è¿‘æ›´æ–°å†…å®¹")
    
    # æŸ¥æ‰¾æ–‡ç« å†…å®¹å¼€å§‹ä½ç½®ï¼ˆé€šå¸¸åœ¨ç¬¬ä¸€ä¸ªh1æˆ–h2æ ‡ç­¾ä¹‹åï¼‰
    article_start = None
    h1_match = re.search(r'<h1[^>]*>(.*?)</h1>', cleaned_content)
    h2_match = re.search(r'<h2[^>]*>(.*?)</h2>', cleaned_content)
    
    if h1_match:
        article_start = h1_match.end()
    elif h2_match:
        article_start = h2_match.end()
    
    if article_start is None:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {os.path.basename(article_path)} ä¸­æ‰¾åˆ°æ–‡ç« å¼€å§‹æ ‡è®°")
        return False
    
    # ç”Ÿæˆæœ€æ–°æ›´æ–°å†…å®¹
    current_year = datetime.datetime.now().year
    current_date = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    content_id = generate_content_id('latest_update', article_path)
    
    # éšæœºé€‰æ‹©æ›´æ–°ç±»å‹
    update_types = [
        f"{current_year}å¹´æœ€æ–°è¶‹åŠ¿",
        f"{current_year}å¹´è¡Œä¸šåŠ¨æ€",
        f"{current_year}å¹´æœ€æ–°æ•°æ®",
        f"{current_year}å¹´æŠ€æœ¯æ›´æ–°",
        f"{current_year}å¹´å¸‚åœºå˜åŒ–"
    ]
    update_type = random.choice(update_types)
    
    # æ ¹æ®æ–‡ç« ç±»å‹ç”Ÿæˆä¸åŒçš„æ›´æ–°å†…å®¹
    article_type = article_config.get('type', 'data')
    
    if article_type == 'core':
        # æ ¸å¿ƒå†…å®¹çš„æ›´æ–°æ›´ä¿å®ˆï¼Œä¸»è¦æ˜¯è¡Œä¸šè¶‹åŠ¿
        update_items = [
            f"AIæŠ€æœ¯æ­£åœ¨æ”¹å˜{random.choice(['å¸‚åœºæ ¼å±€', 'ç”¨æˆ·ä½“éªŒ', 'å¼€å‘æµç¨‹', 'è®¾è®¡ç†å¿µ'])}ï¼Œä¼ä¸šéœ€è¦ç§¯æé€‚åº”ã€‚",
            f"æ ¹æ®æœ€æ–°è°ƒç ”ï¼Œ{random.randint(60, 85)}%çš„ç”¨æˆ·æ›´çœ‹é‡{random.choice(['ç§»åŠ¨ç«¯ä½“éªŒ', 'åŠ è½½é€Ÿåº¦', 'å†…å®¹è´¨é‡', 'äº¤äº’è®¾è®¡'])}ã€‚",
            f"{random.choice(['æ•°æ®é©±åŠ¨å†³ç­–', 'ç”¨æˆ·ä½“éªŒè‡³ä¸Š', 'å…¨æ¸ é“è¥é”€ç­–ç•¥'])}æˆä¸º{current_year}å¹´çš„å…³é”®è¶‹åŠ¿ã€‚"
        ]
    else:
        # æ•°æ®å†…å®¹çš„æ›´æ–°æ›´æ¿€è¿›ï¼ŒåŒ…å«æ›´å¤šæ•°æ®å’Œç»Ÿè®¡
        update_items = [
            f"{current_year}å¹´ç¬¬{random.choice(['ä¸€', 'äºŒ', 'ä¸‰', 'å››'])}å­£åº¦æ•°æ®æ˜¾ç¤ºï¼Œ{random.choice(['ç§»åŠ¨ç«¯æµé‡', 'ç”¨æˆ·åœç•™æ—¶é—´', 'è½¬åŒ–ç‡', 'è·³å‡ºç‡'])}æå‡äº†{random.randint(15, 40)}%ã€‚",
            f"æœ€æ–°ç»Ÿè®¡è¡¨æ˜ï¼Œé‡‡ç”¨{random.choice(['å“åº”å¼è®¾è®¡', 'AIé©±åŠ¨å†…å®¹', 'ä¸ªæ€§åŒ–ç”¨æˆ·ä½“éªŒ', 'å¤šæ¸ é“è¥é”€'])}çš„ç½‘ç«™ï¼Œè½¬åŒ–ç‡å¹³å‡æé«˜{random.randint(20, 50)}%ã€‚",
            f"{random.randint(70, 90)}%çš„æˆåŠŸæ¡ˆä¾‹è¡¨æ˜ï¼Œ{random.choice(['å†…å®¹è´¨é‡', 'é¡µé¢é€Ÿåº¦', 'ç§»åŠ¨å‹å¥½æ€§', 'ç”¨æˆ·ç•Œé¢è®¾è®¡'])}æ˜¯å½±å“æ’åçš„å…³é”®å› ç´ ã€‚"
        ]
    
    # éšæœºé€‰æ‹©æ›´æ–°é¡¹ç›®
    random.shuffle(update_items)
    selected_items = update_items[:random.randint(2, len(update_items))]
    update_content = "\n".join([f"<li>{item}</li>" for item in selected_items])
    
    latest_update_html = f"""
{CONTENT_BLOCK_MARKERS['latest_update']}
<style>
.latest-update-box {{
  background-color: #f8f9fa;
  border-left: 4px solid #4CAF50;
  padding: 15px;
  margin: 20px 0;
  border-radius: 3px;
  box-shadow: 0 1px 3px rgba(0,0,0,0.1);
}}
.latest-update-box h4 {{
  margin-top: 0;
  color: #2E7D32;
  font-weight: 600;
}}
.latest-update-box ul {{
  margin-bottom: 0;
  padding-left: 20px;
}}
.latest-update-box .update-date {{
  font-size: 0.85em;
  color: #666;
  margin-top: 10px;
  text-align: right;
}}
</style>
<div class="latest-update-box" data-update-id="{content_id}">
  <h4>ğŸ“Š {update_type}</h4>
  <ul>
    {update_content}
  </ul>
  <div class="update-date">æ›´æ–°æ—¥æœŸ: {current_date}</div>
</div>
{CONTENT_BLOCK_MARKERS['latest_update_end']}
"""
    
    # æ’å…¥æœ€æ–°æ›´æ–°åŒºå—åˆ°æ–‡ç« å¼€å§‹ä½ç½®ä¹‹å
    updated_content = cleaned_content[:article_start] + latest_update_html + cleaned_content[article_start:]
    
    # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
    if updated_content != content:
        log_message(f"å·²åœ¨æ–‡ç«  {os.path.basename(article_path)} ä¸­æ·»åŠ æ–°çš„æœ€è¿‘æ›´æ–°åŒºå— (ID: {content_id})")
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(updated_content)
        return True
    
    return False

def insert_new_content(article_path, article_config):
    """åœ¨æ–‡ç« ç°æœ‰å†…å®¹ä¸­æ’å…¥æ–°çš„æ®µè½å’Œæ•°æ®ï¼Œè€Œéå®Œå…¨æ›¿æ¢"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        article_title = 'æ–‡ç« '
    else:
        article_title = title_match.group(1)
    
    # æ¸…ç†å·²æœ‰çš„è§è§£åŒºå—ï¼ˆä½¿ç”¨æ ‡è®°ç³»ç»Ÿï¼‰
    content, cleaned_marked = clean_marked_blocks(content, 'new_insight')
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡è®°çš„åŒºå—ï¼Œå°è¯•ä½¿ç”¨æ—§æ–¹æ³•æ¸…ç†
    if not cleaned_marked:
        original_length = len(content)
        log_message(f"å¼€å§‹æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æ—§è§è§£åŒºå—")
        
        # å°è¯•æŸ¥æ‰¾å¹¶åˆ é™¤å·²æœ‰çš„æ–°è§è§£åŒºå—
        insight_box_pattern = r'<div[^>]*class=["\']new-insight-box["\'][^>]*>[\s\S]*?</div>\s*'
        insight_style_pattern = r'<style>\s*\.new-insight-box[\s\S]*?</style>\s*'
        
        # å°è¯•æŒ‰æœ€ä¸¥æ ¼çš„æ–¹å¼åŒ¹é…å®Œæ•´çš„åŒºå—
        full_pattern = insight_box_pattern + insight_style_pattern
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å®Œæ•´æ¨¡å¼ï¼Œåˆ™å°è¯•åˆ†åˆ«åŒ¹é…å’Œåˆ é™¤
        if re.search(full_pattern, content):
            content = re.sub(full_pattern, '', content)
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„å®Œæ•´è§è§£åŒºå—")
        else:
            # å…ˆåˆ é™¤å†…å®¹æ¡†
            if re.search(insight_box_pattern, content):
                content = re.sub(insight_box_pattern, '', content)
                log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„è§è§£åŒºå—æ¡†")
            
            # å†åˆ é™¤æ ·å¼
            if re.search(insight_style_pattern, content):
                content = re.sub(insight_style_pattern, '', content)
                log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„è§è§£åŒºå—æ ·å¼")
        
        # å¦‚æœè¿˜æœ‰å…¶ä»–åŒ…å«"æœ€æ–°è¶‹åŠ¿åˆ†æ"æˆ–"å¸¸è§é—®é¢˜è§£ç­”"çš„åŒºå—ï¼Œç»§ç»­æ¸…ç†
        trend_pattern = r'<div[^>]*>[\s\S]*?æœ€æ–°è¶‹åŠ¿åˆ†æ[\s\S]*?</div>\s*'
        faq_pattern = r'<div[^>]*class=["\']faq-section["\'][^>]*>[\s\S]*?</div>\s*'
        
        # æ¸…ç†è¶‹åŠ¿åˆ†æåŒºå—
        cleaned_count = 0
        while re.search(trend_pattern, content):
            old_content = content
            content = re.sub(trend_pattern, '', content, count=1)
            if len(old_content) > len(content):
                cleaned_count += 1
        
        if cleaned_count > 0:
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„é¢å¤–è¶‹åŠ¿åˆ†æåŒºå—ï¼Œå…± {cleaned_count} ä¸ª")
        
        # æ¸…ç†FAQåŒºå—
        cleaned_count = 0
        while re.search(faq_pattern, content):
            old_content = content
            content = re.sub(faq_pattern, '', content, count=1)
            if len(old_content) > len(content):
                cleaned_count += 1
        
        if cleaned_count > 0:
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„é¢å¤–FAQåŒºå—ï¼Œå…± {cleaned_count} ä¸ª")
        
        bytes_removed = original_length - len(content)
        if bytes_removed > 0:
            log_message(f"æ–‡ç«  {article_path} ä¸­å…±æ¸…ç†äº† {bytes_removed} å­—èŠ‚çš„æ—§è§è§£å†…å®¹")
        else:
            log_message(f"æ–‡ç«  {article_path} ä¸­æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ—§è§è§£å†…å®¹")
    
    # æŸ¥æ‰¾æ–‡ç« ä¸­çš„h2æˆ–h3æ ‡ç­¾ä½ç½®ï¼Œç”¨äºæ’å…¥æ–°å†…å®¹
    h2_matches = list(re.finditer(r'<h[23]>.*?</h[23]>', content))
    if not h2_matches or len(h2_matches) < 2:
        log_message(f"æ–‡ä»¶ {article_path} ä¸­æ²¡æœ‰è¶³å¤Ÿçš„æ ‡é¢˜æ ‡è®°ç”¨äºæ’å…¥å†…å®¹")
        return False
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªh2/h3æ ‡é¢˜åæ’å…¥æ–°å†…å®¹
    insert_pos = random.choice(h2_matches[1:]).end()  # è·³è¿‡ç¬¬ä¸€ä¸ªæ ‡é¢˜
    
    # ç”Ÿæˆéšæœºæ•°æ®
    current_year = datetime.datetime.now().year
    traffic_increase = random.randint(65, 80)
    growth_rate = random.randint(5, 15)
    stay_time_increase = random.randint(25, 40)
    conversion_rate = random.randint(20, 35)
    
    # æå–å…³é”®è¯
    keywords = article_config.get('keywords', [])
    if not keywords:
        keywords = extract_keywords(content)
        article_config['keywords'] = keywords
    
    # éšæœºé€‰æ‹©1-2ä¸ªå…³é”®è¯å¼ºåŒ–
    enhanced_keywords = []
    if keywords:
        num_keywords = min(len(keywords), random.randint(1, 2))
        enhanced_keywords = random.sample(keywords, num_keywords)
    
    # ç”Ÿæˆæ–°çš„æ’å…¥å†…å®¹ï¼Œæ·»åŠ FAQç»“æ„å’Œæ ‡è®°
    content_id = generate_content_id('new_insight', article_path)
    
    new_insight = f'''
            {CONTENT_BLOCK_MARKERS['new_insight']}
            <div class="new-insight-box" data-insight-id="{content_id}">
                <h4>{current_year}å¹´æœ€æ–°è¶‹åŠ¿åˆ†æ</h4>
                <p>éšç€æŠ€æœ¯çš„å¿«é€Ÿè¿­ä»£ï¼Œ{article_title.split(':')[0] if ':' in article_title else article_title}é¢†åŸŸå‡ºç°äº†æ–°çš„å‘å±•è¶‹åŠ¿ï¼š</p>
                
                <div class="trend-data">
                    <ul>
                        <li>ç§»åŠ¨ç«¯è®¿é—®æ¯”ä¾‹è¾¾åˆ°{traffic_increase}%ï¼ŒåŒæ¯”å¢é•¿{growth_rate}%</li>
                        <li>ç”¨æˆ·å¹³å‡åœç•™æ—¶é—´æå‡{stay_time_increase}%</li>
                        <li>å®æ–½ç°ä»£åŒ–ç­–ç•¥çš„ä¼ä¸šè½¬åŒ–ç‡æå‡{conversion_rate}%</li>
    '''
    
    # æ·»åŠ å…³é”®è¯å¼ºåŒ–éƒ¨åˆ†
    if enhanced_keywords:
        keyword_insights = []
        for kw in enhanced_keywords:
            insight = f"{kw}ç›¸å…³æŠ€æœ¯çš„åº”ç”¨æ•ˆæœæå‡äº†{random.randint(15, 40)}%"
            keyword_insights.append(insight)
        
        new_insight += f'''
                        <li>{'ï¼Œ'.join(keyword_insights)}</li>
        '''
    
    new_insight += f'''
                    </ul>
                </div>
                
                <!-- æ·»åŠ FAQç»“æ„ï¼Œå¸¦æœ‰ç»“æ„åŒ–æ•°æ®æ ‡è®° -->
                <div class="faq-section" itemscope itemtype="https://schema.org/FAQPage">
                    <h4>å¸¸è§é—®é¢˜è§£ç­”</h4>
                    <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                        <h5 itemprop="name">å¦‚ä½•æé«˜ç§»åŠ¨ç«¯ç”¨æˆ·ä½“éªŒï¼Ÿ</h5>
                        <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                            <div itemprop="text">
                                <p>æé«˜ç§»åŠ¨ç«¯ç”¨æˆ·ä½“éªŒçš„å…³é”®ç­–ç•¥åŒ…æ‹¬ï¼šä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦ã€é‡‡ç”¨å“åº”å¼è®¾è®¡ã€ç®€åŒ–å¯¼èˆªç»“æ„ã€å¢å¤§è§¦æ‘¸ç›®æ ‡å°ºå¯¸ï¼Œä»¥åŠç¡®ä¿å†…å®¹æ˜“äºé˜…è¯»ã€‚å®šæœŸè¿›è¡Œç”¨æˆ·æµ‹è¯•å¹¶æ ¹æ®Core Web VitalsæŒ‡æ ‡è¿›è¡Œä¼˜åŒ–ä¹Ÿè‡³å…³é‡è¦ã€‚</p>
                            </div>
                        </div>
                    </div>
                    <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                        <h5 itemprop="name">æœ€æ–°çš„SEOè¶‹åŠ¿æœ‰å“ªäº›ï¼Ÿ</h5>
                        <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                            <div itemprop="text">
                                <p>æœ€æ–°çš„SEOè¶‹åŠ¿åŒ…æ‹¬ï¼šç§»åŠ¨ä¼˜å…ˆç´¢å¼•ã€é¡µé¢ä½“éªŒä¿¡å·(Core Web Vitals)ã€è¯­ä¹‰æœç´¢å’Œæ„å›¾åŒ¹é…ã€AIå†…å®¹ä¼˜åŒ–ã€è§†é¢‘å†…å®¹çš„é‡è¦æ€§æå‡ï¼Œä»¥åŠç»“æ„åŒ–æ•°æ®çš„å¹¿æ³›åº”ç”¨ã€‚å…³æ³¨ç”¨æˆ·ä½“éªŒå’Œé«˜è´¨é‡å†…å®¹ä»ç„¶æ˜¯SEOçš„åŸºç¡€ã€‚</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <style>
                .new-insight-box {{
                    background-color: #f0f8ff;
                    border: 1px solid #d1e7ff;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 5px;
                }}
                .trend-data {{
                    margin: 10px 0;
                }}
                .faq-section {{
                    margin-top: 25px;
                    border-top: 1px solid #e0e0e0;
                    padding-top: 15px;
                }}
                .faq-item {{
                    margin-bottom: 15px;
                }}
                .faq-item h5 {{
                    margin-bottom: 8px;
                    color: #2c3e50;
                    font-weight: 600;
                }}
            </style>
            {CONTENT_BLOCK_MARKERS['new_insight_end']}
    '''
    
    # æ’å…¥æ–°å†…å®¹
    new_content = content[:insert_pos] + new_insight + content[insert_pos:]
    log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ æ–°çš„è§è§£åŒºå— (ID: {content_id})")
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

# æ–°å¢çš„SEOä¼˜åŒ–åŠŸèƒ½
def add_internal_links(article_path, article_config, all_articles):
    """æ·»åŠ ç›¸å…³æ–‡ç« çš„å†…éƒ¨é“¾æ¥"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸å…³æ–‡ç« åŒºå—
    if has_marked_block(content, 'related_articles'):
        # å·²å­˜åœ¨ç›¸å…³æ–‡ç« ï¼Œæ¯æ¬¡æ›´æ–°éƒ½æ¸…ç†å’Œé‡å»º
        content, cleaned = clean_marked_blocks(content, 'related_articles')
        if not cleaned:
            log_message(f"è­¦å‘Šï¼šæ‰¾åˆ°ç›¸å…³æ–‡ç« æ ‡è®°ä½†æ— æ³•æ¸…ç†ï¼Œæ–‡ä»¶: {article_path}")
    
    # è·å–å½“å‰æ–‡ç« çš„å…³é”®è¯
    current_keywords = article_config.get('keywords', [])
    if not current_keywords:
        current_keywords = extract_keywords(content)
        article_config['keywords'] = current_keywords
    
    # æŸ¥æ‰¾å¯èƒ½çš„ç›¸å…³æ–‡ç« 
    related_articles = []
    current_filename = os.path.basename(article_path)
    
    for other_article in all_articles:
        other_filename = other_article['file']
        
        # è·³è¿‡å½“å‰æ–‡ç« 
        if other_filename == current_filename:
            continue
        
        # è·å–å…¶ä»–æ–‡ç« çš„å…³é”®è¯
        other_keywords = other_article.get('keywords', [])
        if not other_keywords:
            other_article_path = normalize_path(os.path.join(ARTICLES_DIR, other_filename))
            if os.path.exists(other_article_path):
                with open(other_article_path, 'r', encoding='utf-8') as f:
                    other_content = f.read()
                other_keywords = extract_keywords(other_content)
                other_article['keywords'] = other_keywords
        
        # è®¡ç®—å…³é”®è¯ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼šå…±åŒå…³é”®è¯æ•°é‡ï¼‰
        common_keywords = set(current_keywords).intersection(set(other_keywords))
        if common_keywords:
            # è·å–æ–‡ç« æ ‡é¢˜
            other_article_path = normalize_path(os.path.join(ARTICLES_DIR, other_filename))
            if os.path.exists(other_article_path):
                with open(other_article_path, 'r', encoding='utf-8') as f:
                    other_content = f.read()
                title_match = re.search(r'<h1>(.*?)</h1>', other_content)
                if title_match:
                    article_title = title_match.group(1)
                    related_articles.append({
                        'file': other_filename,
                        'title': article_title,
                        'common_keywords': len(common_keywords)
                    })
    
    # æŒ‰ç›¸å…³æ€§æ’åºå¹¶é€‰æ‹©å‰3ç¯‡
    related_articles.sort(key=lambda x: x['common_keywords'], reverse=True)
    top_related = related_articles[:3] if len(related_articles) > 3 else related_articles
    
    if not top_related:
        return False
    
    # ç”Ÿæˆç›¸å…³æ–‡ç« åŒºå—ID
    content_id = generate_content_id('related_articles', article_path)
    
    # åœ¨æ–‡ç« åº•éƒ¨æ·»åŠ ç›¸å…³æ–‡ç« é“¾æ¥ï¼Œå¸¦æ ‡è®°
    related_links_section = f'''
            {CONTENT_BLOCK_MARKERS['related_articles']}
            <div class="related-articles" data-related-id="{content_id}">
                <h3>ç›¸å…³æ¨è</h3>
                <ul>
    '''
    
    for related in top_related:
        related_links_section += f'''
                    <li><a href="{related['file']}">{related['title']}</a></li>
        '''
    
    related_links_section += f'''
                </ul>
            </div>
            
            <style>
                .related-articles {{
                    background-color: #f9f9f9;
                    padding: 15px;
                    margin: 30px 0;
                    border-radius: 5px;
                    border-top: 2px solid #e0e0e0;
                }}
                .related-articles h3 {{
                    margin-top: 0;
                    color: #333;
                }}
                .related-articles ul {{
                    padding-left: 20px;
                }}
                .related-articles li {{
                    margin-bottom: 8px;
                }}
            </style>
            {CONTENT_BLOCK_MARKERS['related_articles_end']}
    '''
    
    # æŸ¥æ‰¾æ–‡ç« ç»“æŸä½ç½®
    end_marker = '</article>'
    end_pos = content.find(end_marker)
    if end_pos == -1:
        end_marker = '</div>'
        end_pos = content.rfind(end_marker)
    
    if end_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« ç»“æŸæ ‡è®°")
        return False
    
    # æ’å…¥ç›¸å…³æ–‡ç« é“¾æ¥
    new_content = content[:end_pos] + related_links_section + content[end_pos:]
    log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ ç›¸å…³æ–‡ç« é“¾æ¥ (ID: {content_id})")
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def add_schema_markup(article_path, article_config):
    """æ·»åŠ Schema.orgç»“æ„åŒ–æ•°æ®æ ‡è®°"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æ„åŒ–æ•°æ®ï¼ˆé€šè¿‡æ ‡è®°æ£€æŸ¥ï¼‰
    if has_marked_block(content, 'schema_markup'):
        content, cleaned = clean_marked_blocks(content, 'schema_markup')
        if not cleaned:
            log_message(f"è­¦å‘Šï¼šæ‰¾åˆ°ç»“æ„åŒ–æ•°æ®æ ‡è®°ä½†æ— æ³•æ¸…ç†ï¼Œæ–‡ä»¶: {article_path}")
    elif 'itemtype="https://schema.org/Article"' in content or 'application/ld+json' in content:
        # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†æ—§çš„ç»“æ„åŒ–æ•°æ®
        old_schema_pattern = r'<script type="application/ld\+json">[\s\S]*?</script>'
        content = re.sub(old_schema_pattern, '', content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æ—§ç»“æ„åŒ–æ•°æ®")
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« æ ‡é¢˜")
        return False
    
    article_title = title_match.group(1)
    
    # è·å–æ–‡ç« æ—¥æœŸ
    date_match = re.search(r'<span class="article-date"><i class="far fa-calendar-alt"></i>\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)</span>', content)
    article_date = date_match.group(1) if date_match else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    # è·å–æ–‡ç« æè¿°ï¼ˆä½¿ç”¨ç¬¬ä¸€æ®µè½ä½œä¸ºæè¿°ï¼‰
    description_match = re.search(r'<p>(.*?)</p>', content)
    article_description = ''
    if description_match:
        article_description = re.sub(r'<.*?>', '', description_match.group(1))
        if len(article_description) > 160:
            article_description = article_description[:157] + '...'
    
    # è·å–ä½œè€…ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    author_match = re.search(r'<span class="article-author">(.*?)</span>', content)
    article_author = author_match.group(1) if author_match else 'ç½‘ç«™ç®¡ç†å‘˜'
    
    # ç”Ÿæˆç»“æ„åŒ–æ•°æ®ID
    content_id = generate_content_id('schema_markup', article_path)
    
    # æ„å»ºç»“æ„åŒ–æ•°æ®ï¼Œå¸¦æ ‡è®°
    schema_markup = f'''
    {CONTENT_BLOCK_MARKERS['schema_markup']}
    <script type="application/ld+json" data-schema-id="{content_id}">
    {{"@context":"https://schema.org",
      "@type":"Article",
      "headline":"{article_title}",
      "description":"{article_description}",
      "author":{{"@type":"Person","name":"{article_author}"}},
      "publisher":{{"@type":"Organization","name":"ç½‘ç«™åç§°","logo":{{"@type":"ImageObject","url":"logo.png"}}}},
      "datePublished":"{article_date}",
      "dateModified":"{datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}"
    }}
    </script>
    {CONTENT_BLOCK_MARKERS['schema_markup_end']}
    '''
    
    # æŸ¥æ‰¾</head>æ ‡ç­¾ä½ç½®
    head_end_pos = content.find('</head>')
    if head_end_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°</head>æ ‡ç­¾")
        return False
    
    # æ’å…¥ç»“æ„åŒ–æ•°æ®
    new_content = content[:head_end_pos] + schema_markup + content[head_end_pos:]
    log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ ç»“æ„åŒ–æ•°æ® (ID: {content_id})")
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def optimize_images(article_path):
    """ä¼˜åŒ–æ–‡ç« ä¸­çš„å›¾ç‰‡ï¼ˆæ·»åŠ altæ ‡ç­¾ã€å‹ç¼©å›¾ç‰‡ï¼‰"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–æ–‡ç« æ ‡é¢˜å’Œå…³é”®è¯ï¼Œç”¨äºç”Ÿæˆaltæ ‡ç­¾
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    article_title = title_match.group(1) if title_match else ''
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
    img_tags = re.findall(r'<img\s+[^>]*src=["\']([^"\'>]+)["\'][^>]*>', content)
    if not img_tags:
        return False
    
    # ç¡®ä¿å›¾ç‰‡ç›®å½•å­˜åœ¨
    if not os.path.exists(IMAGES_DIR):
        os.makedirs(IMAGES_DIR)
    
    modified = False
    for img_src in img_tags:
        # è·³è¿‡å¤–éƒ¨å›¾ç‰‡å’Œå·²ç»ä¼˜åŒ–è¿‡çš„å›¾ç‰‡
        if img_src.startswith('http') or 'optimized_' in img_src:
            continue
        
        # æ„å»ºå›¾ç‰‡è·¯å¾„
        img_path = normalize_path(os.path.join(os.path.dirname(article_path), img_src))
        if not os.path.exists(img_path):
            continue
        
        try:
            # ç”Ÿæˆä¼˜åŒ–åçš„å›¾ç‰‡åç§°
            img_name = os.path.basename(img_src)
            img_ext = os.path.splitext(img_name)[1].lower()
            optimized_img_name = f"optimized_{img_name}"
            optimized_img_path = normalize_path(os.path.join(IMAGES_DIR, optimized_img_name))
            
            # ä¼˜åŒ–å›¾ç‰‡å°ºå¯¸å’Œè´¨é‡
            with Image.open(img_path) as img:
                # è°ƒæ•´å›¾ç‰‡å°ºå¯¸
                if img.width > MAX_IMAGE_WIDTH:
                    ratio = MAX_IMAGE_WIDTH / img.width
                    new_height = int(img.height * ratio)
                    img = img.resize((MAX_IMAGE_WIDTH, new_height), Image.LANCZOS)
                
                # ä¿å­˜ä¼˜åŒ–åçš„å›¾ç‰‡
                if img_ext in ['.jpg', '.jpeg']:
                    img.save(optimized_img_path, 'JPEG', quality=IMAGE_QUALITY, optimize=True)
                elif img_ext == '.png':
                    img.save(optimized_img_path, 'PNG', optimize=True)
                else:
                    img.save(optimized_img_path)
            
            # ç”Ÿæˆaltæ ‡ç­¾ï¼ˆä½¿ç”¨æ–‡ç« æ ‡é¢˜å’Œå›¾ç‰‡åç§°ï¼‰
            img_name_clean = os.path.splitext(img_name)[0].replace('-', ' ').replace('_', ' ')
            alt_text = f"{article_title} - {img_name_clean}"
            
            # æ›¿æ¢å›¾ç‰‡æ ‡ç­¾ï¼Œæ·»åŠ altå±æ€§å’Œloading="lazy"å±æ€§
            old_img_tag = re.search(r'<img\s+[^>]*src=["\']' + re.escape(img_src) + r'["\'][^>]*>', content)
            if old_img_tag:
                old_tag = old_img_tag.group(0)
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰altå±æ€§
                if 'alt=' not in old_tag:
                    new_tag = old_tag.replace('<img ', f'<img alt="{alt_text}" ')
                else:
                    new_tag = old_tag
                
                # æ·»åŠ loading="lazy"å±æ€§
                if 'loading=' not in new_tag:
                    new_tag = new_tag.replace('<img ', '<img loading="lazy" ')
                
                # æ›´æ–°srcå±æ€§
                relative_path = os.path.relpath(optimized_img_path, os.path.dirname(article_path)).replace('\\', '/')
                new_tag = re.sub(r'src=["\'][^"\'>]+["\']', f'src="{relative_path}"', new_tag)
                
                # æ›¿æ¢æ ‡ç­¾
                content = content.replace(old_tag, new_tag)
                modified = True
        except Exception as e:
            log_message(f"ä¼˜åŒ–å›¾ç‰‡æ—¶å‡ºé”™: {img_path}, é”™è¯¯: {str(e)}")
    
    if modified:
        # å†™å›æ–‡ä»¶
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    
    return False

def enhance_mobile_seo(article_path):
    """å¢å¼ºç§»åŠ¨ç«¯SEOï¼Œæé«˜Core Web Vitalsåˆ†æ•°"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç§»åŠ¨ç«¯ä¼˜åŒ–ï¼ˆé€šè¿‡æ ‡è®°æ£€æŸ¥ï¼‰
    if has_marked_block(content, 'mobile_style'):
        return False  # ç§»åŠ¨ç«¯æ ·å¼åªéœ€æ·»åŠ ä¸€æ¬¡ï¼Œä¸éœ€è¦æ¯æ¬¡æ›´æ–°
    
    modified = False
    
    # 1. ç¡®ä¿æœ‰viewportå…ƒæ ‡ç­¾
    if 'viewport' not in content:
        head_end_pos = content.find('</head>')
        if head_end_pos != -1:
            viewport_meta = '<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">\n    '
            content = content[:head_end_pos] + viewport_meta + content[head_end_pos:]
            modified = True
    
    # 2. æ·»åŠ ç§»åŠ¨ç«¯ä¼˜åŒ–CSS
    if 'mobile-optimization' not in content:
        # ç”Ÿæˆå†…å®¹ID
        content_id = generate_content_id('mobile_style', article_path)
        
        style_section = f'''
        {CONTENT_BLOCK_MARKERS['mobile_style']}
        <style class="mobile-optimization" data-mobile-id="{content_id}">
            /* ç§»åŠ¨ç«¯ä¼˜åŒ–æ ·å¼ */
            @media (max-width: 768px) {{
                body {{
                    font-size: 16px;
                    line-height: 1.6;
                }}
                h1 {{
                    font-size: 24px;
                    line-height: 1.3;
                }}
                h2 {{
                    font-size: 20px;
                }}
                h3 {{
                    font-size: 18px;
                }}
                .container, .content {{
                    padding-left: 15px;
                    padding-right: 15px;
                }}
                img {{
                    max-width: 100%;
                    height: auto;
                }}
                /* æ”¹å–„è§¦æ‘¸ç›®æ ‡å°ºå¯¸ */
                a, button {{
                    min-height: 44px;
                    min-width: 44px;
                }}
                /* æ”¹å–„è¡¨å•å…ƒç´ åœ¨ç§»åŠ¨ç«¯çš„å¯ç”¨æ€§ */
                input, select, textarea {{
                    font-size: 16px; /* é˜²æ­¢iOSç¼©æ”¾ */
                }}
            }}
        </style>
        {CONTENT_BLOCK_MARKERS['mobile_style_end']}
        '''
        
        head_end_pos = content.find('</head>')
        if head_end_pos != -1:
            content = content[:head_end_pos] + style_section + content[head_end_pos:]
            modified = True
            log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ ç§»åŠ¨ç«¯ä¼˜åŒ–æ ·å¼ (ID: {content_id})")
    
    if modified:
        # å†™å›æ–‡ä»¶
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    
    return False

def add_social_meta_tags(article_path):
    """æ·»åŠ ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾ï¼ˆOpen Graphå’ŒTwitter Cardï¼‰"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¤¾äº¤åª’ä½“æ ‡ç­¾ï¼ˆé€šè¿‡æ ‡è®°æ£€æŸ¥ï¼‰
    if has_marked_block(content, 'social_meta'):
        content, cleaned = clean_marked_blocks(content, 'social_meta')
        if not cleaned:
            log_message(f"è­¦å‘Šï¼šæ‰¾åˆ°ç¤¾äº¤åª’ä½“æ ‡è®°ä½†æ— æ³•æ¸…ç†ï¼Œæ–‡ä»¶: {article_path}")
    elif 'og:title' in content or 'twitter:card' in content:
        # å°è¯•ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¸…ç†æ—§çš„ç¤¾äº¤åª’ä½“æ ‡ç­¾
        og_pattern = r'<meta property="og:[^"]*"[^>]*>'
        twitter_pattern = r'<meta name="twitter:[^"]*"[^>]*>'
        
        content = re.sub(og_pattern, '', content)
        content = re.sub(twitter_pattern, '', content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æ—§ç¤¾äº¤åª’ä½“æ ‡ç­¾")
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« æ ‡é¢˜")
        return False
    
    article_title = title_match.group(1)
    
    # è·å–æ–‡ç« æè¿°ï¼ˆä½¿ç”¨ç¬¬ä¸€æ®µè½ä½œä¸ºæè¿°ï¼‰
    description_match = re.search(r'<p>(.*?)</p>', content)
    article_description = ''
    if description_match:
        article_description = re.sub(r'<.*?>', '', description_match.group(1))
        if len(article_description) > 160:
            article_description = article_description[:157] + '...'
    
    # æŸ¥æ‰¾æ–‡ç« ä¸­çš„ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºç¤¾äº¤åª’ä½“å›¾ç‰‡
    img_match = re.search(r'<img\s+[^>]*src=["\']([^"\'>]+)["\'][^>]*>', content)
    article_image = ''
    if img_match:
        article_image = img_match.group(1)
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹URLï¼ˆå‡è®¾ç½‘ç«™æ ¹ç›®å½•ï¼‰
        if not article_image.startswith('http'):
            article_image = f"/{article_image.lstrip('/')}" if article_image else ''
    
    # ç”Ÿæˆç¤¾äº¤åª’ä½“æ ‡ç­¾ID
    content_id = generate_content_id('social_meta', article_path)
    
    # æ„å»ºç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾ï¼Œå¸¦æ ‡è®°
    social_meta_tags = f'''
    {CONTENT_BLOCK_MARKERS['social_meta']}
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="{article_title}">
    <meta property="og:description" content="{article_description}">
    <meta property="og:url" content="{urllib.parse.quote(os.path.basename(article_path))}">
    '''
    
    if article_image:
        social_meta_tags += f'''
    <meta property="og:image" content="{article_image}">
        '''
    
    social_meta_tags += f'''
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="{article_title}">
    <meta name="twitter:description" content="{article_description}">
    '''
    
    if article_image:
        social_meta_tags += f'''
    <meta name="twitter:image" content="{article_image}">
        '''
    
    social_meta_tags += f'''
    {CONTENT_BLOCK_MARKERS['social_meta_end']}
    '''
    
    # æŸ¥æ‰¾</head>æ ‡ç­¾ä½ç½®
    head_end_pos = content.find('</head>')
    if head_end_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°</head>æ ‡ç­¾")
        return False
    
    # æ’å…¥ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾
    new_content = content[:head_end_pos] + social_meta_tags + content[head_end_pos:]
    log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ ç¤¾äº¤åª’ä½“æ ‡ç­¾ (ID: {content_id})")
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def cleanup_all_blocks(article_path):
    """æ¸…ç†æ–‡ç« ä¸­æ‰€æœ‰çš„è‡ªåŠ¨ç”ŸæˆåŒºå—"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    original_size = len(content)
    blocks_cleaned = 0
    
    # æ¸…ç†æ‰€æœ‰ç±»å‹çš„åŒºå—
    for block_type in CONTENT_BLOCK_MARKERS:
        if block_type.endswith('_end'):  # è·³è¿‡ç»“æŸæ ‡è®°
            continue
        
        cleaned_content, cleaned = clean_marked_blocks(content, block_type)
        if cleaned:
            content = cleaned_content
            blocks_cleaned += 1
    
    # å¦‚æœå†…å®¹è¢«ä¿®æ”¹ï¼Œä¿å­˜æ–‡ä»¶
    if blocks_cleaned > 0:
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(content)
        
        bytes_removed = original_size - len(content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„ {blocks_cleaned} ä¸ªåŒºå—ï¼Œå‡å°‘ {bytes_removed} å­—èŠ‚")
        return True
    
    return False

def scan_for_duplicate_blocks(article_path):
    """æ‰«ææ–‡ç« ä¸­æ˜¯å¦å­˜åœ¨é‡å¤çš„åŒºå—æ ‡è®°"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    duplicates_found = False
    
    # æ£€æŸ¥æ¯ç§ç±»å‹çš„åŒºå—
    for block_type in CONTENT_BLOCK_MARKERS:
        if block_type.endswith('_end'):  # è·³è¿‡ç»“æŸæ ‡è®°
            continue
        
        start_marker = CONTENT_BLOCK_MARKERS[block_type]
        matches = re.findall(re.escape(start_marker), content)
        
        if len(matches) > 1:
            duplicates_found = True
            log_message(f"è­¦å‘Šï¼šæ–‡ç«  {article_path} ä¸­å‘ç° {len(matches)} ä¸ª '{block_type}' åŒºå—æ ‡è®°")
    
    return duplicates_found

def update_wechat_popup(article_path):
    """ç¡®ä¿å¾®ä¿¡å›¾æ ‡ç‚¹å‡»ååªæ˜¾ç¤ºäºŒç»´ç çª—å£ï¼Œä¸æ˜¾ç¤ºå…¶ä»–å†…å®¹"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    modified = False
    
    # é¦–å…ˆæŸ¥æ‰¾å¹¶å®Œå…¨åˆ é™¤æ‰€æœ‰å¯èƒ½çš„å¾®ä¿¡å¼¹çª—
    wechat_modal_start_pattern = r'<div\s+id="wechat-modal"'
    wechat_modal_starts = [m.start() for m in re.finditer(wechat_modal_start_pattern, content)]
    
    if wechat_modal_starts:
        # å¦‚æœæ‰¾åˆ°å¾®ä¿¡å¼¹çª—ï¼Œåˆ é™¤æ‰€æœ‰çš„
        log_message(f"åœ¨æ–‡ç«  {article_path} ä¸­æ‰¾åˆ° {len(wechat_modal_starts)} ä¸ªå¾®ä¿¡å¼¹çª—ï¼Œå‡†å¤‡æ¸…ç†")
        
        # åˆ›å»ºä¸€ä¸ªæ–°çš„å†…å®¹å­—ç¬¦ä¸²ï¼Œæ’é™¤æ‰€æœ‰å¾®ä¿¡å¼¹çª—
        new_content = content
        for start_pos in sorted(wechat_modal_starts, reverse=True):  # ä»åå‘å‰å¤„ç†
            # æ‰¾åˆ°å¼¹çª—å¼€å§‹çš„div
            div_count = 1
            end_pos = start_pos
            
            # å‘åæŸ¥æ‰¾åŒ¹é…çš„</div>æ ‡ç­¾
            while div_count > 0 and end_pos < len(content):
                end_pos += 1
                if content.find('<div', end_pos, end_pos + 20) == end_pos:
                    div_count += 1
                elif content.find('</div>', end_pos, end_pos + 20) == end_pos:
                    div_count -= 1
            
            if div_count == 0:  # æ‰¾åˆ°äº†åŒ¹é…çš„ç»“æŸæ ‡ç­¾
                # åˆ é™¤æ•´ä¸ªå¼¹çª—
                modal_content = content[start_pos:end_pos + 6]  # +6 æ˜¯</div>çš„é•¿åº¦
                new_content = new_content.replace(modal_content, '')
                log_message(f"å·²æ¸…ç†ä¸€ä¸ªå¾®ä¿¡å¼¹çª—ï¼Œé•¿åº¦: {len(modal_content)} å­—èŠ‚")
        
        content = new_content
        modified = True
    
    # æ ‡å‡†çš„å¾®ä¿¡å¼¹çª—ç»“æ„
    wechat_popup = '''
    <!-- å¾®ä¿¡äºŒç»´ç å¼¹çª— -->
    <div id="wechat-modal" class="wechat-modal wechat-popup">
        <div class="wechat-modal-content">
            <span class="close-modal">&times;</span>
            <h3>æ‰«æäºŒç»´ç æ·»åŠ å¾®ä¿¡</h3>
            <div class="qrcode-container">
                <img loading="lazy" src="../images/optimized_wechat-qrcode.jpg" alt="å¾®ä¿¡äºŒç»´ç " width="200" height="200">
            </div>
            <p>å¾®ä¿¡å·: pds051207</p>
        </div>
    </div>
    '''
    
    # åœ¨</body>æ ‡ç­¾å‰æ’å…¥æ ‡å‡†çš„å¾®ä¿¡å¼¹çª—
    body_end_pos = content.rfind('</body>')
    if body_end_pos != -1:
        content = content[:body_end_pos] + wechat_popup + content[body_end_pos:]
        log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ æ ‡å‡†å¾®ä¿¡å¼¹çª—")
        modified = True
    
    # ç¡®ä¿å¾®ä¿¡å¼¹çª—CSSè¢«æ­£ç¡®å¼•ç”¨
    if '<link rel="stylesheet" href="../wechat-popup.css">' not in content:
        head_end_pos = content.find('</head>')
        if head_end_pos != -1:
            content = content[:head_end_pos] + '\n    <!-- å¾®ä¿¡å¼¹çª—æ ·å¼ -->\n    <link rel="stylesheet" href="../wechat-popup.css">' + content[head_end_pos:]
            modified = True
    
    # ç¡®ä¿å¾®ä¿¡å¼¹çª—JSè¢«æ­£ç¡®å¼•ç”¨
    if '<script src="../wechat-popup.js"></script>' not in content:
        body_end_pos = content.rfind('</body>')
        if body_end_pos != -1:
            content = content[:body_end_pos] + '\n    <!-- å¾®ä¿¡å¼¹çª—è„šæœ¬ -->\n    <script src="../wechat-popup.js"></script>' + content[body_end_pos:]
            modified = True
    
    # å†™å›æ–‡ä»¶
    if modified:
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    
    return False

def create_wechat_popup_files():
    """ç¡®ä¿wechat-popup.jså’Œwechat-popup.cssæ–‡ä»¶å­˜åœ¨å¹¶åŒ…å«æ­£ç¡®çš„å†…å®¹"""
    # åˆ›å»ºwechat-popup.cssæ–‡ä»¶
    css_content = """/* å¾®ä¿¡å¼¹çª—æ ·å¼ */
.wechat-modal {
    display: none;
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    background-color: rgba(0, 0, 0, 0.7);
    z-index: 9999; /* ç¡®ä¿åœ¨æœ€ä¸Šå±‚ */
    justify-content: center;
    align-items: center;
}

.wechat-modal-content {
    background-color: white;
    padding: 30px;
    border-radius: 10px;
    text-align: center;
    position: relative;
    max-width: 90%;
    width: 350px;
    box-shadow: 0 5px 20px rgba(0,0,0,0.2);
    animation: fadeIn 0.3s ease-out;
}

@keyframes fadeIn {
    from { opacity: 0; transform: translateY(-20px); }
    to { opacity: 1; transform: translateY(0); }
}

.close-modal {
    position: absolute;
    top: 10px;
    right: 15px;
    font-size: 24px;
    cursor: pointer;
    color: #999;
    transition: color 0.3s;
}

.close-modal:hover {
    color: #333;
}

.qrcode-container {
    margin: 20px 0;
    display: flex;
    justify-content: center;
}

.qrcode-container img {
    max-width: 100%;
    height: auto;
    border: 1px solid #eee;
}

.wechat-modal h3 {
    color: #333;
    margin-top: 0;
    font-size: 18px;
}

.wechat-modal p {
    margin-bottom: 0;
    color: #666;
    font-size: 16px;
}

/* ç¡®ä¿å¼¹çª—ä¸è¢«å…¶ä»–å…ƒç´ é®æŒ¡ */
.wechat-popup {
    z-index: 9999;
} 
"""

    # åˆ›å»ºwechat-popup.jsæ–‡ä»¶
    js_content = """// å¾®ä¿¡äºŒç»´ç å¼¹çª—è„šæœ¬
document.addEventListener('DOMContentLoaded', function() {
    // è·å–å¾®ä¿¡å¼¹çª—å…ƒç´ 
    const wechatModal = document.getElementById('wechat-modal');
    if (!wechatModal) {
        console.error('æœªæ‰¾åˆ°å¾®ä¿¡å¼¹çª—å…ƒç´ ï¼ŒIDä¸ºwechat-modal');
        return;
    }
    
    // è·å–å…³é—­æŒ‰é’®
    const closeModal = wechatModal.querySelector('.close-modal');
    
    // æ˜¾ç¤ºå¼¹çª—çš„å‡½æ•°
    function showWechatModal(e) {
        if (e) e.preventDefault();
        wechatModal.style.display = 'flex';
        document.body.style.overflow = 'hidden'; // é˜²æ­¢èƒŒæ™¯æ»šåŠ¨
    }
    
    // å…³é—­å¼¹çª—çš„å‡½æ•°
    function closeWechatModal() {
        wechatModal.style.display = 'none';
        document.body.style.overflow = '';
    }
    
    // ä¸ºé¡µé¢ä¸Šæ‰€æœ‰å¾®ä¿¡ç›¸å…³é“¾æ¥æ·»åŠ ç‚¹å‡»äº‹ä»¶
    // 1. é€šè¿‡IDæŸ¥æ‰¾å¾®ä¿¡é“¾æ¥
    const wechatLinks = [
        document.getElementById('wechat-link'),
        document.getElementById('footer-wechat-link'),
        document.getElementById('article-wechat-link')
    ].filter(Boolean); // è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„å…ƒç´ 
    
    // 2. é€šè¿‡ç±»åå’Œæ ‡é¢˜æŸ¥æ‰¾å¾®ä¿¡é“¾æ¥
    document.querySelectorAll('.social-link[title="åˆ†äº«åˆ°å¾®ä¿¡"], .social-link[title="å¾®ä¿¡"], .fab.fa-weixin').forEach(function(link) {
        link.addEventListener('click', showWechatModal);
    });
    
    // 3. ä¸ºæ‰€æœ‰å¾®ä¿¡å›¾æ ‡æ·»åŠ äº‹ä»¶ï¼ˆä¸ä¾èµ–äºç‰¹å®šIDæˆ–æ ‡é¢˜ï¼‰
    document.querySelectorAll('.fab.fa-weixin').forEach(function(icon) {
        // æ‰¾åˆ°åŒ…å«æ­¤å›¾æ ‡çš„æœ€è¿‘çš„aæ ‡ç­¾
        const parentLink = icon.closest('a');
        if (parentLink) {
            parentLink.addEventListener('click', showWechatModal);
        }
    });
    
    // 4. ä¸ºæ‰¾åˆ°çš„IDé“¾æ¥æ·»åŠ äº‹ä»¶
    wechatLinks.forEach(function(link) {
        link.addEventListener('click', showWechatModal);
    });
    
    // 5. ä¸ºæ–‡ç« é¡µé¢ä¸­çš„ç¤¾äº¤åˆ†äº«æŒ‰é’®æ·»åŠ äº‹ä»¶
    document.querySelectorAll('.share-buttons a').forEach(function(link) {
        if (link.querySelector('.fa-weixin') || link.querySelector('.fab.fa-weixin')) {
            link.addEventListener('click', showWechatModal);
        }
    });
    
    // ä¸ºå…³é—­æŒ‰é’®æ·»åŠ ç‚¹å‡»äº‹ä»¶
    if (closeModal) {
        closeModal.addEventListener('click', closeWechatModal);
    }
    
    // ç‚¹å‡»å¼¹çª—å¤–éƒ¨å…³é—­å¼¹çª—
    window.addEventListener('click', function(e) {
        if (e.target === wechatModal) {
            closeWechatModal();
        }
    });
});
"""

    # å†™å…¥CSSæ–‡ä»¶
    with open('wechat-popup.css', 'w', encoding='utf-8') as f:
        f.write(css_content)
    log_message("å·²åˆ›å»ºæˆ–æ›´æ–°wechat-popup.cssæ–‡ä»¶")

    # å†™å…¥JSæ–‡ä»¶
    with open('wechat-popup.js', 'w', encoding='utf-8') as f:
        f.write(js_content)
    log_message("å·²åˆ›å»ºæˆ–æ›´æ–°wechat-popup.jsæ–‡ä»¶")
    
    # ç¡®ä¿imagesç›®å½•å­˜åœ¨
    if not os.path.exists('images'):
        os.makedirs('images')
        log_message("å·²åˆ›å»ºimagesç›®å½•")
    
    # æ£€æŸ¥å¾®ä¿¡äºŒç»´ç å›¾ç‰‡æ˜¯å¦å­˜åœ¨
    qrcode_path = normalize_path(os.path.join('images', 'optimized_wechat-qrcode.jpg'))
    if not os.path.exists(qrcode_path):
        # å¦‚æœå›¾ç‰‡ä¸å­˜åœ¨ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„å ä½å›¾ç‰‡
        try:
            # å°è¯•åŠ è½½ç³»ç»Ÿå­—ä½“ï¼Œæ ¹æ®ä¸åŒæ“ä½œç³»ç»Ÿé€‰æ‹©ä¸åŒçš„å­—ä½“
            if platform.system() == 'Windows':
                font = ImageFont.truetype("arial.ttf", 20)
            elif platform.system() == 'Linux':
                # Linuxç³»ç»Ÿå¸¸ç”¨å­—ä½“è·¯å¾„
                font_paths = [
                    "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf",
                    "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
                    "/usr/share/fonts/truetype/freefont/FreeSans.ttf"
                ]
                font = None
                for font_path in font_paths:
                    try:
                        if os.path.exists(font_path):
                            font = ImageFont.truetype(font_path, 20)
                            break
                    except:
                        continue
                if font is None:
                    font = ImageFont.load_default()
            else:
                # macOSæˆ–å…¶ä»–ç³»ç»Ÿ
                try:
                    font = ImageFont.truetype("Arial.ttf", 20)
                except:
                    font = ImageFont.load_default()
            
            # åˆ›å»ºä¸€ä¸ª200x200çš„ç™½è‰²å›¾ç‰‡
            img = Image.new('RGB', (200, 200), color='white')
            draw = ImageDraw.Draw(img)
            
            # ç»˜åˆ¶è¾¹æ¡†
            draw.rectangle([(0, 0), (199, 199)], outline='black')
            
            # æ·»åŠ æ–‡å­—
            draw.text((40, 80), "å¾®ä¿¡äºŒç»´ç ", fill='black', font=font)
            draw.text((30, 110), "è¯·æ›¿æ¢ä¸ºå®é™…å›¾ç‰‡", fill='black', font=font)
            
            # ä¿å­˜å›¾ç‰‡
            img.save(qrcode_path, 'JPEG', quality=95)
            log_message(f"å·²åˆ›å»ºå¾®ä¿¡äºŒç»´ç å ä½å›¾: {qrcode_path}")
        except Exception as e:
            log_message(f"åˆ›å»ºå¾®ä¿¡äºŒç»´ç å ä½å›¾å¤±è´¥: {str(e)}")
    
    return True

def normalize_path(path):
    """æ ‡å‡†åŒ–è·¯å¾„ï¼Œç¡®ä¿åœ¨ä¸åŒæ“ä½œç³»ç»Ÿä¸Šä½¿ç”¨æ­£ç¡®çš„è·¯å¾„åˆ†éš”ç¬¦"""
    return os.path.normpath(path)

def update_articles():
    """æ›´æ–°éœ€è¦æ›´æ–°çš„æ–‡ç« """
    config = load_config()
    today = datetime.datetime.now().strftime('%Y-%m-%d')
    updated_count = 0
    
    # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    
    # ç¡®ä¿å›¾ç‰‡ç›®å½•å­˜åœ¨
    if not os.path.exists(IMAGES_DIR):
        os.makedirs(IMAGES_DIR)
    
    for article in config['articles']:
        if should_update_article(article):
            article_path = normalize_path(os.path.join(ARTICLES_DIR, article['file']))
            
            if not os.path.exists(article_path):
                log_message(f"æ–‡ä»¶ä¸å­˜åœ¨: {article_path}")
                continue
            
            # æ‰«ææ£€æŸ¥é‡å¤åŒºå—
            if scan_for_duplicate_blocks(article_path):
                log_message(f"æ–‡ç«  {article_path} å­˜åœ¨é‡å¤åŒºå—ï¼Œè¿›è¡Œå®Œå…¨æ¸…ç†")
                cleanup_all_blocks(article_path)
            
            # å¤‡ä»½æ–‡ç« 
            backup_path = backup_article(article_path)
            
            # æ›´æ–°æ–‡ç« æ—¥æœŸ
            date_updated = update_article_date(article_path)
            
            # æ ¹æ®æ–‡ç« ç±»å‹é€‰æ‹©æ›´æ–°ç­–ç•¥
            article_type = article.get('type', 'data')
            if article_type == 'core':
                # æ ¸å¿ƒå†…å®¹ï¼šåªæ·»åŠ æœ€æ–°æ›´æ–°åŒºå—ï¼Œä¸ä¿®æ”¹ä¸»ä½“
                content_updated = add_latest_update_section(article_path, article)
            else:
                # æ•°æ®å†…å®¹ï¼šæ·»åŠ æœ€æ–°æ›´æ–°åŒºå—å¹¶åœ¨æ­£æ–‡ä¸­æ’å…¥æ–°å†…å®¹
                update_section_added = add_latest_update_section(article_path, article)
                new_content_inserted = insert_new_content(article_path, article)
                content_updated = update_section_added or new_content_inserted
            
            # åº”ç”¨SEOä¼˜åŒ–
            log_message(f"å¼€å§‹å¯¹æ–‡ç«  {article['file']} åº”ç”¨SEOä¼˜åŒ–...")
            
            # 1. æ·»åŠ å†…éƒ¨é“¾æ¥ç»“æ„
            internal_links_added = add_internal_links(article_path, article, config['articles'])
            if internal_links_added:
                log_message(f"å·²æ·»åŠ å†…éƒ¨é“¾æ¥: {article['file']}")
            
            # 2. æ·»åŠ Schema.orgç»“æ„åŒ–æ•°æ®æ ‡è®°
            schema_added = add_schema_markup(article_path, article)
            if schema_added:
                log_message(f"å·²æ·»åŠ ç»“æ„åŒ–æ•°æ®æ ‡è®°: {article['file']}")
            
            # 3. ä¼˜åŒ–å›¾ç‰‡ï¼ˆæ·»åŠ altæ ‡ç­¾ã€å‹ç¼©å›¾ç‰‡ï¼‰
            images_optimized = optimize_images(article_path)
            if images_optimized:
                log_message(f"å·²ä¼˜åŒ–å›¾ç‰‡: {article['file']}")
            
            # 4. å¢å¼ºç§»åŠ¨ç«¯SEO
            mobile_enhanced = enhance_mobile_seo(article_path)
            if mobile_enhanced:
                log_message(f"å·²å¢å¼ºç§»åŠ¨ç«¯SEO: {article['file']}")
            
            # 5. æ·»åŠ ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾
            social_tags_added = add_social_meta_tags(article_path)
            if social_tags_added:
                log_message(f"å·²æ·»åŠ ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾: {article['file']}")
            
            # 6. æ›´æ–°å¾®ä¿¡å¼¹çª—ï¼Œç¡®ä¿ç‚¹å‡»å¾®ä¿¡å›¾æ ‡åªæ˜¾ç¤ºäºŒç»´ç 
            wechat_popup_updated = update_wechat_popup(article_path)
            if wechat_popup_updated:
                log_message(f"å·²æ›´æ–°å¾®ä¿¡å¼¹çª—: {article['file']}")
            
            # æœ€åå†æ¬¡æ‰«ææ£€æŸ¥æ˜¯å¦æœ‰é‡å¤åŒºå—
            has_duplicates = scan_for_duplicate_blocks(article_path)
            if has_duplicates:
                log_message(f"è­¦å‘Šï¼šæ›´æ–°åæ–‡ç«  {article_path} ä»å­˜åœ¨é‡å¤åŒºå—ï¼Œè¿™å¯èƒ½éœ€è¦æ‰‹åŠ¨æ£€æŸ¥")
            
            # æ›´æ–°æ–‡ç« çŠ¶æ€
            if date_updated or content_updated or internal_links_added or schema_added or \
               images_optimized or mobile_enhanced or social_tags_added or wechat_popup_updated:
                article['last_updated'] = today
                updated_count += 1
                log_message(f"å·²å®Œæˆæ–‡ç« æ›´æ–°å’ŒSEOä¼˜åŒ–: {article['file']} (ç±»å‹: {article_type})")
            else:
                log_message(f"æ›´æ–°å¤±è´¥: {article['file']}")
    
    # ä¿å­˜æ›´æ–°åçš„é…ç½®
    save_config(config)
    log_message(f"å®Œæˆæ›´æ–°ï¼Œå…±æ›´æ–° {updated_count} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    try:
        log_message("å¼€å§‹è‡ªåŠ¨æ›´æ–°æ–‡ç« ...")
        
        # ç¡®ä¿å¾®ä¿¡å¼¹çª—ç›¸å…³æ–‡ä»¶å­˜åœ¨
        create_wechat_popup_files()
        
        update_articles()
        
        # æ›´æ–°sitemap.xml
        log_message("å¼€å§‹æ›´æ–°sitemap.xml...")
        sitemap_updated = update_sitemap()
        if sitemap_updated:
            log_message("sitemap.xmlæ›´æ–°å®Œæˆ")
        else:
            log_message("sitemap.xmlæ›´æ–°å¤±è´¥")
        
        log_message("æ‰€æœ‰æ›´æ–°å®Œæˆ")
    except Exception as e:
        log_message(f"æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        log_message(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        
        # å°è¯•æ¸…ç†æ‰€æœ‰æ–‡ç« ä¸­å¯èƒ½ç•™ä¸‹çš„ä¸å®Œæ•´åŒºå—
        try:
            log_message("å°è¯•æ¸…ç†å¯èƒ½çš„ä¸å®Œæ•´åŒºå—...")
            config = load_config()
            cleanup_count = 0
            
            for article in config['articles']:
                article_path = normalize_path(os.path.join(ARTICLES_DIR, article['file']))
                if os.path.exists(article_path):
                    if cleanup_all_blocks(article_path):
                        cleanup_count += 1
            
            if cleanup_count > 0:
                log_message(f"å·²æ¸…ç† {cleanup_count} ç¯‡æ–‡ç« ä¸­çš„ä¸å®Œæ•´åŒºå—")
            else:
                log_message("æœªå‘ç°éœ€è¦æ¸…ç†çš„ä¸å®Œæ•´åŒºå—")
        except Exception as cleanup_error:
            log_message(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(cleanup_error)}")
            log_message(f"æ¸…ç†é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
