import os
import random
import datetime
import re
import json
import shutil
import urllib.parse
import sys
import subprocess

# è‡ªåŠ¨å®‰è£…æ‰€éœ€çš„ä¾èµ–
try:
    from PIL import Image
    print("PILå·²æˆåŠŸå¯¼å…¥")
except ImportError:
    print("æ­£åœ¨å®‰è£…PIL/Pillowåº“...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "Pillow"])
    from PIL import Image
    print("PIL/Pillowå®‰è£…å’Œå¯¼å…¥æˆåŠŸ")

from pathlib import Path
from update_sitemap import update_sitemap  # å¯¼å…¥sitemapæ›´æ–°åŠŸèƒ½

# é…ç½®
ARTICLES_DIR = 'articles'
ARTICLES_CONFIG = 'articles_config.json'
LOG_FILE = 'update_log.txt'
BACKUP_DIR = 'articles_backup'  # æ–‡ç« å¤‡ä»½ç›®å½•
IMAGES_DIR = 'images'  # å›¾ç‰‡ç›®å½•
MAX_IMAGE_WIDTH = 1200  # æœ€å¤§å›¾ç‰‡å®½åº¦
IMAGE_QUALITY = 85  # å›¾ç‰‡å‹ç¼©è´¨é‡

def log_message(message):
    """è®°å½•æ—¥å¿—"""
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f'[{timestamp}] {message}\n')
    print(f'[{timestamp}] {message}')

def load_config():
    """åŠ è½½æ–‡ç« é…ç½®"""
    if os.path.exists(ARTICLES_CONFIG):
        with open(ARTICLES_CONFIG, 'r', encoding='utf-8') as f:
            return json.load(f)
    else:
        # é»˜è®¤é…ç½®
        default_config = {
            "articles": [
                {"file": "seo-guide.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "responsive-design.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "ui-ux-design.html", "update_frequency": 2, "last_updated": "", "type": "core", "keywords": []},
                {"file": "website-development.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "mobile-app-development.html", "update_frequency": 2, "last_updated": "", "type": "core", "keywords": []},
                {"file": "ecommerce-solutions.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []},
                {"file": "cloud-services.html", "update_frequency": 2, "last_updated": "", "type": "core", "keywords": []},
                {"file": "conversion-rate.html", "update_frequency": 1, "last_updated": "", "type": "data", "keywords": []}
            ]
        }
        with open(ARTICLES_CONFIG, 'w', encoding='utf-8') as f:
            json.dump(default_config, f, ensure_ascii=False, indent=4)
        return default_config

def save_config(config):
    """ä¿å­˜æ–‡ç« é…ç½®"""
    with open(ARTICLES_CONFIG, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=4)

def should_update_article(article_config):
    """åˆ¤æ–­æ–‡ç« æ˜¯å¦éœ€è¦æ›´æ–°ï¼Œæ ¹æ®æ–‡ç« ç±»å‹è°ƒæ•´æ›´æ–°é¢‘ç‡"""
    if not article_config.get('last_updated'):
        return True
    
    last_updated = datetime.datetime.strptime(article_config['last_updated'], '%Y-%m-%d')
    days_since_update = (datetime.datetime.now() - last_updated).days
    
    # æ ¹æ®æ–‡ç« ç±»å‹è°ƒæ•´æ›´æ–°é¢‘ç‡
    article_type = article_config.get('type', 'data')
    base_frequency = article_config.get('update_frequency', 7)
    
    # æ ¸å¿ƒå†…å®¹æ›´æ–°é¢‘ç‡é™ä½ï¼Œæ•°æ®å†…å®¹æ›´æ–°é¢‘ç‡æé«˜
    if article_type == 'core':
        return days_since_update >= base_frequency * 1.5  # æ ¸å¿ƒå†…å®¹æ›´æ–°é¢‘ç‡é™ä½
    else:
        return days_since_update >= base_frequency  # æ•°æ®å†…å®¹æ­£å¸¸æ›´æ–°

def backup_article(article_path):
    """å¤‡ä»½æ–‡ç« """
    # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½æ–‡ä»¶å
    filename = os.path.basename(article_path)
    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
    backup_filename = f"{os.path.splitext(filename)[0]}_{timestamp}{os.path.splitext(filename)[1]}"
    backup_path = os.path.join(BACKUP_DIR, backup_filename)
    
    # å¤åˆ¶æ–‡ä»¶
    shutil.copy2(article_path, backup_path)
    log_message(f"å·²å¤‡ä»½æ–‡ç« : {filename} -> {backup_filename}")
    return backup_path

def extract_keywords(content):
    """ä»æ–‡ç« å†…å®¹ä¸­æå–å…³é”®è¯"""
    # æå–æ‰€æœ‰h2, h3æ ‡ç­¾å†…å®¹å’ŒåŠ ç²—æ–‡æœ¬ä½œä¸ºå…³é”®è¯
    keywords = []
    
    # æå–æ ‡é¢˜
    h2_matches = re.findall(r'<h2>(.*?)</h2>', content)
    h3_matches = re.findall(r'<h3>(.*?)</h3>', content)
    
    # æå–åŠ ç²—æ–‡æœ¬
    bold_matches = re.findall(r'<strong>(.*?)</strong>', content)
    
    # æå–é•¿å°¾å…³é”®è¯ï¼ˆæ®µè½ä¸­çš„å…³é”®çŸ­è¯­ï¼‰
    p_content = ' '.join(re.findall(r'<p>(.*?)</p>', content))
    p_content = re.sub(r'<.*?>', '', p_content)  # ç§»é™¤HTMLæ ‡ç­¾
    
    # ç®€å•çš„é•¿å°¾å…³é”®è¯æå–ï¼ˆ3-5ä¸ªè¯çš„çŸ­è¯­ï¼‰
    words = p_content.split()
    for i in range(len(words) - 2):
        if i + 4 < len(words):  # 4è¯çŸ­è¯­
            phrase = ' '.join(words[i:i+4])
            if len(phrase) > 10 and not any(char.isdigit() for char in phrase):  # ç®€å•è¿‡æ»¤
                keywords.append(phrase)
    
    # åˆå¹¶å…³é”®è¯
    keywords.extend(h2_matches)
    keywords.extend(h3_matches)
    keywords.extend(bold_matches)
    
    # æ¸…ç†å…³é”®è¯
    cleaned_keywords = []
    for kw in keywords:
        # ç§»é™¤HTMLæ ‡ç­¾
        kw = re.sub(r'<.*?>', '', kw)
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·
        kw = re.sub(r'[^\w\s]', '', kw)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        kw = kw.strip()
        if kw and len(kw) > 1:  # åªä¿ç•™æœ‰æ„ä¹‰çš„å…³é”®è¯
            cleaned_keywords.append(kw)
    
    return list(set(cleaned_keywords))  # å»é‡

def update_article_date(article_path):
    """æ›´æ–°æ–‡ç« çš„å‘å¸ƒæ—¥æœŸ"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ›´æ–°æ–‡ç« æ—¥æœŸ
    today = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    new_content = re.sub(
        r'<span class="article-date"><i class="far fa-calendar-alt"></i>\s*\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥</span>',
        f'<span class="article-date"><i class="far fa-calendar-alt"></i> {today}</span>',
        content
    )
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def add_latest_update_section(article_path, article_config):
    """åœ¨æ–‡ç« é¡¶éƒ¨æ·»åŠ "æœ€æ–°æ›´æ–°"åŒºå—ï¼Œä¿ç•™åŸæœ‰å†…å®¹"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        article_title = 'æ–‡ç« '
    else:
        article_title = title_match.group(1)
    
    # æŸ¥æ‰¾æ–‡ç« ä¸»ä½“å†…å®¹åŒºåŸŸå¼€å§‹ä½ç½®
    start_marker = '</h1>'
    start_pos = content.find(start_marker)
    if start_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« å¼€å§‹æ ‡è®°")
        return False
    
    start_pos += len(start_marker)
    
    # é¦–å…ˆæ£€æŸ¥å¹¶åˆ é™¤å·²æœ‰çš„"æœ€è¿‘æ›´æ–°"åŒºå—ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„åŒ¹é…æ–¹å¼
    original_length = len(content)
    log_message(f"å¼€å§‹æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æ—§æœ€è¿‘æ›´æ–°åŒºå—")
    
    # å°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«"æœ€è¿‘æ›´æ–°"æ–‡å­—çš„divå—
    update_box_pattern = r'<div[^>]*class=["\']latest-update-box["\'][^>]*>[\s\S]*?æœ€æ–°æ›´æ–°[\s\S]*?</div>\s*'
    style_pattern = r'<style>\s*\.latest-update-box[\s\S]*?</style>\s*'
    
    # å°è¯•æŒ‰æœ€ä¸¥æ ¼çš„æ–¹å¼åŒ¹é…å®Œæ•´çš„æ›´æ–°åŒºå—
    full_pattern = update_box_pattern + style_pattern
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å®Œæ•´æ¨¡å¼ï¼Œåˆ™å°è¯•åˆ†åˆ«åŒ¹é…å’Œåˆ é™¤
    if re.search(full_pattern, content):
        content = re.sub(full_pattern, '', content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„å®Œæ•´æœ€è¿‘æ›´æ–°åŒºå—")
    else:
        # å…ˆåˆ é™¤æ›´æ–°æ¡†
        if re.search(update_box_pattern, content):
            content = re.sub(update_box_pattern, '', content)
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æœ€è¿‘æ›´æ–°åŒºå—æ¡†")
        
        # å†åˆ é™¤æ ·å¼
        if re.search(style_pattern, content):
            content = re.sub(style_pattern, '', content)
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æœ€è¿‘æ›´æ–°æ ·å¼")
    
    # å¦‚æœè¿˜æœ‰å…¶ä»–åŒ…å«"æœ€æ–°æ›´æ–°"çš„åŒºå—ï¼Œç»§ç»­æ¸…ç†
    additional_pattern = r'<div[^>]*>[\s\S]*?æœ€æ–°æ›´æ–°[\s\S]*?</div>\s*'
    cleaned_count = 0
    while re.search(additional_pattern, content):
        old_content = content
        content = re.sub(additional_pattern, '', content, count=1)
        if len(old_content) > len(content):
            cleaned_count += 1
    
    if cleaned_count > 0:
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„é¢å¤–æœ€è¿‘æ›´æ–°åŒºå—ï¼Œå…± {cleaned_count} ä¸ª")
    
    bytes_removed = original_length - len(content)
    if bytes_removed > 0:
        log_message(f"æ–‡ç«  {article_path} ä¸­å…±æ¸…ç†äº† {bytes_removed} å­—èŠ‚çš„æ—§æœ€è¿‘æ›´æ–°å†…å®¹")
    else:
        log_message(f"æ–‡ç«  {article_path} ä¸­æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ—§æœ€è¿‘æ›´æ–°å†…å®¹")
    
    # ç”Ÿæˆéšæœºæ•°æ®ï¼ˆç¡®ä¿æ¯ç¯‡æ–‡ç« ä½¿ç”¨ç›¸åŒçš„æ•°æ®ï¼‰
    current_year = datetime.datetime.now().year
    traffic_increase = random.randint(65, 80)
    growth_rate = random.randint(5, 15)
    stay_time_increase = random.randint(25, 40)
    conversion_rate = random.randint(20, 35)
    
    # æå–å…³é”®è¯å¹¶å¢å¼º
    keywords = article_config.get('keywords', [])
    if not keywords:
        keywords = extract_keywords(content)
        article_config['keywords'] = keywords
    
    # éšæœºé€‰æ‹©2-3ä¸ªå…³é”®è¯å¼ºåŒ–
    enhanced_keywords = []
    if keywords:
        num_keywords = min(len(keywords), random.randint(2, 3))
        enhanced_keywords = random.sample(keywords, num_keywords)
    
    # ç”Ÿæˆæ–°çš„æ›´æ–°åŒºå—
    update_date = datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    latest_update_section = f'''
            <div class="latest-update-box">
                <h3>ğŸ”” æœ€æ–°æ›´æ–° ({update_date})</h3>
                <p>æˆ‘ä»¬å¯¹æœ¬æ–‡è¿›è¡Œäº†æ›´æ–°ï¼Œä»¥åæ˜ {article_title.split(':')[0] if ':' in article_title else article_title}é¢†åŸŸçš„æœ€æ–°å‘å±•ï¼š</p>
                
                <div class="update-highlights">
                    <ul>
                        <li><strong>æœ€æ–°æ•°æ®</strong>ï¼š{current_year}å¹´ç§»åŠ¨ç«¯è®¿é—®æ¯”ä¾‹è¾¾åˆ°{traffic_increase}%ï¼ŒåŒæ¯”å¢é•¿{growth_rate}%</li>
                        <li><strong>ç”¨æˆ·ä½“éªŒ</strong>ï¼šå®æ–½ç°ä»£åŒ–ç­–ç•¥çš„ä¼ä¸šç”¨æˆ·åœç•™æ—¶é—´æå‡{stay_time_increase}%</li>
                        <li><strong>è½¬åŒ–æ•ˆæœ</strong>ï¼šä¼˜åŒ–åçš„æ–¹æ¡ˆå¹³å‡è½¬åŒ–ç‡æå‡{conversion_rate}%</li>
    '''
    
    # æ·»åŠ å…³é”®è¯å¼ºåŒ–éƒ¨åˆ†
    if enhanced_keywords:
        latest_update_section += f'''
                        <li><strong>æ–°è¶‹åŠ¿</strong>ï¼š{current_year}å¹´{', '.join(enhanced_keywords)}é¢†åŸŸå‡ºç°é‡å¤§çªç ´</li>
        '''
    
    latest_update_section += '''
                    </ul>
                </div>
                <p><em>ç»§ç»­é˜…è¯»è·å–å®Œæ•´åˆ†æå’Œå®æ–½å»ºè®®...</em></p>
            </div>
            
            <style>
                .latest-update-box {
                    background-color: #f8f9fa;
                    border-left: 4px solid #4CAF50;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 3px;
                }
                .update-highlights {
                    margin: 10px 0;
                }
                .update-highlights ul {
                    margin-bottom: 0;
                }
            </style>
    '''
    
    # é‡æ–°æŸ¥æ‰¾æ–‡ç« ä¸»ä½“å†…å®¹åŒºåŸŸå¼€å§‹ä½ç½®ï¼ˆå› ä¸ºæˆ‘ä»¬å·²ç»åˆ é™¤äº†æ—§çš„æ›´æ–°åŒºå—ï¼‰
    start_marker = '</h1>'
    start_pos = content.find(start_marker)
    if start_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« å¼€å§‹æ ‡è®°")
        return False
    
    start_pos += len(start_marker)
    
    # æ’å…¥æ›´æ–°åŒºå—
    new_content = content[:start_pos] + latest_update_section + content[start_pos:]
    log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ æ–°çš„æœ€è¿‘æ›´æ–°åŒºå—")
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def insert_new_content(article_path, article_config):
    """åœ¨æ–‡ç« ç°æœ‰å†…å®¹ä¸­æ’å…¥æ–°çš„æ®µè½å’Œæ•°æ®ï¼Œè€Œéå®Œå…¨æ›¿æ¢"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        article_title = 'æ–‡ç« '
    else:
        article_title = title_match.group(1)
    
    # å…ˆæ£€æŸ¥å¹¶åˆ é™¤å·²æœ‰çš„æ–°è§è§£åŒºå—
    original_length = len(content)
    log_message(f"å¼€å§‹æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„æ—§è§è§£åŒºå—")
    
    # å°è¯•æŸ¥æ‰¾å¹¶åˆ é™¤å·²æœ‰çš„æ–°è§è§£åŒºå—
    insight_box_pattern = r'<div[^>]*class=["\']new-insight-box["\'][^>]*>[\s\S]*?</div>\s*'
    insight_style_pattern = r'<style>\s*\.new-insight-box[\s\S]*?</style>\s*'
    
    # å°è¯•æŒ‰æœ€ä¸¥æ ¼çš„æ–¹å¼åŒ¹é…å®Œæ•´çš„åŒºå—
    full_pattern = insight_box_pattern + insight_style_pattern
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å®Œæ•´æ¨¡å¼ï¼Œåˆ™å°è¯•åˆ†åˆ«åŒ¹é…å’Œåˆ é™¤
    if re.search(full_pattern, content):
        content = re.sub(full_pattern, '', content)
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„å®Œæ•´è§è§£åŒºå—")
    else:
        # å…ˆåˆ é™¤å†…å®¹æ¡†
        if re.search(insight_box_pattern, content):
            content = re.sub(insight_box_pattern, '', content)
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„è§è§£åŒºå—æ¡†")
        
        # å†åˆ é™¤æ ·å¼
        if re.search(insight_style_pattern, content):
            content = re.sub(insight_style_pattern, '', content)
            log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„è§è§£åŒºå—æ ·å¼")
    
    # å¦‚æœè¿˜æœ‰å…¶ä»–åŒ…å«"æœ€æ–°è¶‹åŠ¿åˆ†æ"æˆ–"å¸¸è§é—®é¢˜è§£ç­”"çš„åŒºå—ï¼Œç»§ç»­æ¸…ç†
    trend_pattern = r'<div[^>]*>[\s\S]*?æœ€æ–°è¶‹åŠ¿åˆ†æ[\s\S]*?</div>\s*'
    faq_pattern = r'<div[^>]*class=["\']faq-section["\'][^>]*>[\s\S]*?</div>\s*'
    
    # æ¸…ç†è¶‹åŠ¿åˆ†æåŒºå—
    cleaned_count = 0
    while re.search(trend_pattern, content):
        old_content = content
        content = re.sub(trend_pattern, '', content, count=1)
        if len(old_content) > len(content):
            cleaned_count += 1
    
    if cleaned_count > 0:
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„é¢å¤–è¶‹åŠ¿åˆ†æåŒºå—ï¼Œå…± {cleaned_count} ä¸ª")
    
    # æ¸…ç†FAQåŒºå—
    cleaned_count = 0
    while re.search(faq_pattern, content):
        old_content = content
        content = re.sub(faq_pattern, '', content, count=1)
        if len(old_content) > len(content):
            cleaned_count += 1
    
    if cleaned_count > 0:
        log_message(f"å·²æ¸…ç†æ–‡ç«  {article_path} ä¸­çš„é¢å¤–FAQåŒºå—ï¼Œå…± {cleaned_count} ä¸ª")
    
    bytes_removed = original_length - len(content)
    if bytes_removed > 0:
        log_message(f"æ–‡ç«  {article_path} ä¸­å…±æ¸…ç†äº† {bytes_removed} å­—èŠ‚çš„æ—§è§è§£å†…å®¹")
    else:
        log_message(f"æ–‡ç«  {article_path} ä¸­æœªæ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ—§è§è§£å†…å®¹")
    
    # æŸ¥æ‰¾æ–‡ç« ä¸­çš„h2æˆ–h3æ ‡ç­¾ä½ç½®ï¼Œç”¨äºæ’å…¥æ–°å†…å®¹
    h2_matches = list(re.finditer(r'<h[23]>.*?</h[23]>', content))
    if not h2_matches or len(h2_matches) < 2:
        log_message(f"æ–‡ä»¶ {article_path} ä¸­æ²¡æœ‰è¶³å¤Ÿçš„æ ‡é¢˜æ ‡è®°ç”¨äºæ’å…¥å†…å®¹")
        return False
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªh2/h3æ ‡é¢˜åæ’å…¥æ–°å†…å®¹
    insert_pos = random.choice(h2_matches[1:]).end()  # è·³è¿‡ç¬¬ä¸€ä¸ªæ ‡é¢˜
    
    # ç”Ÿæˆéšæœºæ•°æ®
    current_year = datetime.datetime.now().year
    traffic_increase = random.randint(65, 80)
    growth_rate = random.randint(5, 15)
    stay_time_increase = random.randint(25, 40)
    conversion_rate = random.randint(20, 35)
    
    # æå–å…³é”®è¯
    keywords = article_config.get('keywords', [])
    if not keywords:
        keywords = extract_keywords(content)
        article_config['keywords'] = keywords
    
    # éšæœºé€‰æ‹©1-2ä¸ªå…³é”®è¯å¼ºåŒ–
    enhanced_keywords = []
    if keywords:
        num_keywords = min(len(keywords), random.randint(1, 2))
        enhanced_keywords = random.sample(keywords, num_keywords)
    
    # ç”Ÿæˆæ–°çš„æ’å…¥å†…å®¹ï¼Œæ·»åŠ FAQç»“æ„
    new_insight = f'''
            <div class="new-insight-box">
                <h4>{current_year}å¹´æœ€æ–°è¶‹åŠ¿åˆ†æ</h4>
                <p>éšç€æŠ€æœ¯çš„å¿«é€Ÿè¿­ä»£ï¼Œ{article_title.split(':')[0] if ':' in article_title else article_title}é¢†åŸŸå‡ºç°äº†æ–°çš„å‘å±•è¶‹åŠ¿ï¼š</p>
                
                <div class="trend-data">
                    <ul>
                        <li>ç§»åŠ¨ç«¯è®¿é—®æ¯”ä¾‹è¾¾åˆ°{traffic_increase}%ï¼ŒåŒæ¯”å¢é•¿{growth_rate}%</li>
                        <li>ç”¨æˆ·å¹³å‡åœç•™æ—¶é—´æå‡{stay_time_increase}%</li>
                        <li>å®æ–½ç°ä»£åŒ–ç­–ç•¥çš„ä¼ä¸šè½¬åŒ–ç‡æå‡{conversion_rate}%</li>
    '''
    
    # æ·»åŠ å…³é”®è¯å¼ºåŒ–éƒ¨åˆ†
    if enhanced_keywords:
        keyword_insights = []
        for kw in enhanced_keywords:
            insight = f"{kw}ç›¸å…³æŠ€æœ¯çš„åº”ç”¨æ•ˆæœæå‡äº†{random.randint(15, 40)}%"
            keyword_insights.append(insight)
        
        new_insight += f'''
                        <li>{'ï¼Œ'.join(keyword_insights)}</li>
        '''
    
    new_insight += '''
                    </ul>
                </div>
                
                <!-- æ·»åŠ FAQç»“æ„ï¼Œå¸¦æœ‰ç»“æ„åŒ–æ•°æ®æ ‡è®° -->
                <div class="faq-section" itemscope itemtype="https://schema.org/FAQPage">
                    <h4>å¸¸è§é—®é¢˜è§£ç­”</h4>
                    <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                        <h5 itemprop="name">å¦‚ä½•æé«˜ç§»åŠ¨ç«¯ç”¨æˆ·ä½“éªŒï¼Ÿ</h5>
                        <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                            <div itemprop="text">
                                <p>æé«˜ç§»åŠ¨ç«¯ç”¨æˆ·ä½“éªŒçš„å…³é”®ç­–ç•¥åŒ…æ‹¬ï¼šä¼˜åŒ–é¡µé¢åŠ è½½é€Ÿåº¦ã€é‡‡ç”¨å“åº”å¼è®¾è®¡ã€ç®€åŒ–å¯¼èˆªç»“æ„ã€å¢å¤§è§¦æ‘¸ç›®æ ‡å°ºå¯¸ï¼Œä»¥åŠç¡®ä¿å†…å®¹æ˜“äºé˜…è¯»ã€‚å®šæœŸè¿›è¡Œç”¨æˆ·æµ‹è¯•å¹¶æ ¹æ®Core Web VitalsæŒ‡æ ‡è¿›è¡Œä¼˜åŒ–ä¹Ÿè‡³å…³é‡è¦ã€‚</p>
                            </div>
                        </div>
                    </div>
                    <div class="faq-item" itemscope itemprop="mainEntity" itemtype="https://schema.org/Question">
                        <h5 itemprop="name">æœ€æ–°çš„SEOè¶‹åŠ¿æœ‰å“ªäº›ï¼Ÿ</h5>
                        <div itemscope itemprop="acceptedAnswer" itemtype="https://schema.org/Answer">
                            <div itemprop="text">
                                <p>æœ€æ–°çš„SEOè¶‹åŠ¿åŒ…æ‹¬ï¼šç§»åŠ¨ä¼˜å…ˆç´¢å¼•ã€é¡µé¢ä½“éªŒä¿¡å·(Core Web Vitals)ã€è¯­ä¹‰æœç´¢å’Œæ„å›¾åŒ¹é…ã€AIå†…å®¹ä¼˜åŒ–ã€è§†é¢‘å†…å®¹çš„é‡è¦æ€§æå‡ï¼Œä»¥åŠç»“æ„åŒ–æ•°æ®çš„å¹¿æ³›åº”ç”¨ã€‚å…³æ³¨ç”¨æˆ·ä½“éªŒå’Œé«˜è´¨é‡å†…å®¹ä»ç„¶æ˜¯SEOçš„åŸºç¡€ã€‚</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <style>
                .new-insight-box {
                    background-color: #f0f8ff;
                    border: 1px solid #d1e7ff;
                    padding: 15px;
                    margin: 20px 0;
                    border-radius: 5px;
                }
                .trend-data {
                    margin: 10px 0;
                }
                .faq-section {
                    margin-top: 25px;
                    border-top: 1px solid #e0e0e0;
                    padding-top: 15px;
                }
                .faq-item {
                    margin-bottom: 15px;
                }
                .faq-item h5 {
                    margin-bottom: 8px;
                    color: #2c3e50;
                    font-weight: 600;
                }
            </style>
    '''
    
    # æ’å…¥æ–°å†…å®¹
    new_content = content[:insert_pos] + new_insight + content[insert_pos:]
    log_message(f"å·²åœ¨æ–‡ç«  {article_path} ä¸­æ·»åŠ æ–°çš„è§è§£åŒºå—")
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

# æ–°å¢çš„SEOä¼˜åŒ–åŠŸèƒ½
def add_internal_links(article_path, article_config, all_articles):
    """æ·»åŠ ç›¸å…³æ–‡ç« çš„å†…éƒ¨é“¾æ¥"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–å½“å‰æ–‡ç« çš„å…³é”®è¯
    current_keywords = article_config.get('keywords', [])
    if not current_keywords:
        current_keywords = extract_keywords(content)
        article_config['keywords'] = current_keywords
    
    # æŸ¥æ‰¾å¯èƒ½çš„ç›¸å…³æ–‡ç« 
    related_articles = []
    current_filename = os.path.basename(article_path)
    
    for other_article in all_articles:
        other_filename = other_article['file']
        
        # è·³è¿‡å½“å‰æ–‡ç« 
        if other_filename == current_filename:
            continue
        
        # è·å–å…¶ä»–æ–‡ç« çš„å…³é”®è¯
        other_keywords = other_article.get('keywords', [])
        if not other_keywords:
            other_article_path = os.path.join(ARTICLES_DIR, other_filename)
            if os.path.exists(other_article_path):
                with open(other_article_path, 'r', encoding='utf-8') as f:
                    other_content = f.read()
                other_keywords = extract_keywords(other_content)
                other_article['keywords'] = other_keywords
        
        # è®¡ç®—å…³é”®è¯ç›¸ä¼¼åº¦ï¼ˆç®€å•å®ç°ï¼šå…±åŒå…³é”®è¯æ•°é‡ï¼‰
        common_keywords = set(current_keywords).intersection(set(other_keywords))
        if common_keywords:
            # è·å–æ–‡ç« æ ‡é¢˜
            other_article_path = os.path.join(ARTICLES_DIR, other_filename)
            if os.path.exists(other_article_path):
                with open(other_article_path, 'r', encoding='utf-8') as f:
                    other_content = f.read()
                title_match = re.search(r'<h1>(.*?)</h1>', other_content)
                if title_match:
                    article_title = title_match.group(1)
                    related_articles.append({
                        'file': other_filename,
                        'title': article_title,
                        'common_keywords': len(common_keywords)
                    })
    
    # æŒ‰ç›¸å…³æ€§æ’åºå¹¶é€‰æ‹©å‰3ç¯‡
    related_articles.sort(key=lambda x: x['common_keywords'], reverse=True)
    top_related = related_articles[:3] if len(related_articles) > 3 else related_articles
    
    if not top_related:
        return False
    
    # åœ¨æ–‡ç« åº•éƒ¨æ·»åŠ ç›¸å…³æ–‡ç« é“¾æ¥
    related_links_section = '''
            <div class="related-articles">
                <h3>ç›¸å…³æ¨è</h3>
                <ul>
    '''
    
    for related in top_related:
        related_links_section += f'''
                    <li><a href="{related['file']}">{related['title']}</a></li>
        '''
    
    related_links_section += '''
                </ul>
            </div>
            
            <style>
                .related-articles {
                    background-color: #f9f9f9;
                    padding: 15px;
                    margin: 30px 0;
                    border-radius: 5px;
                    border-top: 2px solid #e0e0e0;
                }
                .related-articles h3 {
                    margin-top: 0;
                    color: #333;
                }
                .related-articles ul {
                    padding-left: 20px;
                }
                .related-articles li {
                    margin-bottom: 8px;
                }
            </style>
    '''
    
    # æŸ¥æ‰¾æ–‡ç« ç»“æŸä½ç½®
    end_marker = '</article>'
    end_pos = content.find(end_marker)
    if end_pos == -1:
        end_marker = '</div>'
        end_pos = content.rfind(end_marker)
    
    if end_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« ç»“æŸæ ‡è®°")
        return False
    
    # æ’å…¥ç›¸å…³æ–‡ç« é“¾æ¥
    new_content = content[:end_pos] + related_links_section + content[end_pos:]
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def add_schema_markup(article_path, article_config):
    """æ·»åŠ Schema.orgç»“æ„åŒ–æ•°æ®æ ‡è®°"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç»“æ„åŒ–æ•°æ®
    if 'itemtype="https://schema.org/Article"' in content:
        return False
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« æ ‡é¢˜")
        return False
    
    article_title = title_match.group(1)
    
    # è·å–æ–‡ç« æ—¥æœŸ
    date_match = re.search(r'<span class="article-date"><i class="far fa-calendar-alt"></i>\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)</span>', content)
    article_date = date_match.group(1) if date_match else datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    # è·å–æ–‡ç« æè¿°ï¼ˆä½¿ç”¨ç¬¬ä¸€æ®µè½ä½œä¸ºæè¿°ï¼‰
    description_match = re.search(r'<p>(.*?)</p>', content)
    article_description = ''
    if description_match:
        article_description = re.sub(r'<.*?>', '', description_match.group(1))
        if len(article_description) > 160:
            article_description = article_description[:157] + '...'
    
    # è·å–ä½œè€…ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    author_match = re.search(r'<span class="article-author">(.*?)</span>', content)
    article_author = author_match.group(1) if author_match else 'ç½‘ç«™ç®¡ç†å‘˜'
    
    # æ„å»ºç»“æ„åŒ–æ•°æ®
    schema_markup = f'''
    <script type="application/ld+json">
    {{"@context":"https://schema.org",
      "@type":"Article",
      "headline":"{article_title}",
      "description":"{article_description}",
      "author":{{"@type":"Person","name":"{article_author}"}},
      "publisher":{{"@type":"Organization","name":"ç½‘ç«™åç§°","logo":{{"@type":"ImageObject","url":"logo.png"}}}},
      "datePublished":"{article_date}",
      "dateModified":"{datetime.datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}"
    }}
    </script>
    '''
    
    # æŸ¥æ‰¾</head>æ ‡ç­¾ä½ç½®
    head_end_pos = content.find('</head>')
    if head_end_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°</head>æ ‡ç­¾")
        return False
    
    # æ’å…¥ç»“æ„åŒ–æ•°æ®
    new_content = content[:head_end_pos] + schema_markup + content[head_end_pos:]
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def optimize_images(article_path):
    """ä¼˜åŒ–æ–‡ç« ä¸­çš„å›¾ç‰‡ï¼ˆæ·»åŠ altæ ‡ç­¾ã€å‹ç¼©å›¾ç‰‡ï¼‰"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # è·å–æ–‡ç« æ ‡é¢˜å’Œå…³é”®è¯ï¼Œç”¨äºç”Ÿæˆaltæ ‡ç­¾
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    article_title = title_match.group(1) if title_match else ''
    
    # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡æ ‡ç­¾
    img_tags = re.findall(r'<img\s+[^>]*src=["\']([^"\'>]+)["\'][^>]*>', content)
    if not img_tags:
        return False
    
    # ç¡®ä¿å›¾ç‰‡ç›®å½•å­˜åœ¨
    if not os.path.exists(IMAGES_DIR):
        os.makedirs(IMAGES_DIR)
    
    modified = False
    for img_src in img_tags:
        # è·³è¿‡å¤–éƒ¨å›¾ç‰‡å’Œå·²ç»ä¼˜åŒ–è¿‡çš„å›¾ç‰‡
        if img_src.startswith('http') or 'optimized_' in img_src:
            continue
        
        # æ„å»ºå›¾ç‰‡è·¯å¾„
        img_path = os.path.join(os.path.dirname(article_path), img_src)
        if not os.path.exists(img_path):
            continue
        
        try:
            # ç”Ÿæˆä¼˜åŒ–åçš„å›¾ç‰‡åç§°
            img_name = os.path.basename(img_src)
            img_ext = os.path.splitext(img_name)[1].lower()
            optimized_img_name = f"optimized_{img_name}"
            optimized_img_path = os.path.join(IMAGES_DIR, optimized_img_name)
            
            # ä¼˜åŒ–å›¾ç‰‡å°ºå¯¸å’Œè´¨é‡
            with Image.open(img_path) as img:
                # è°ƒæ•´å›¾ç‰‡å°ºå¯¸
                if img.width > MAX_IMAGE_WIDTH:
                    ratio = MAX_IMAGE_WIDTH / img.width
                    new_height = int(img.height * ratio)
                    img = img.resize((MAX_IMAGE_WIDTH, new_height), Image.LANCZOS)
                
                # ä¿å­˜ä¼˜åŒ–åçš„å›¾ç‰‡
                if img_ext in ['.jpg', '.jpeg']:
                    img.save(optimized_img_path, 'JPEG', quality=IMAGE_QUALITY, optimize=True)
                elif img_ext == '.png':
                    img.save(optimized_img_path, 'PNG', optimize=True)
                else:
                    img.save(optimized_img_path)
            
            # ç”Ÿæˆaltæ ‡ç­¾ï¼ˆä½¿ç”¨æ–‡ç« æ ‡é¢˜å’Œå›¾ç‰‡åç§°ï¼‰
            img_name_clean = os.path.splitext(img_name)[0].replace('-', ' ').replace('_', ' ')
            alt_text = f"{article_title} - {img_name_clean}"
            
            # æ›¿æ¢å›¾ç‰‡æ ‡ç­¾ï¼Œæ·»åŠ altå±æ€§å’Œloading="lazy"å±æ€§
            old_img_tag = re.search(r'<img\s+[^>]*src=["\']' + re.escape(img_src) + r'["\'][^>]*>', content)
            if old_img_tag:
                old_tag = old_img_tag.group(0)
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰altå±æ€§
                if 'alt=' not in old_tag:
                    new_tag = old_tag.replace('<img ', f'<img alt="{alt_text}" ')
                else:
                    new_tag = old_tag
                
                # æ·»åŠ loading="lazy"å±æ€§
                if 'loading=' not in new_tag:
                    new_tag = new_tag.replace('<img ', '<img loading="lazy" ')
                
                # æ›´æ–°srcå±æ€§
                relative_path = os.path.relpath(optimized_img_path, os.path.dirname(article_path)).replace('\\', '/')
                new_tag = re.sub(r'src=["\'][^"\'>]+["\']', f'src="{relative_path}"', new_tag)
                
                # æ›¿æ¢æ ‡ç­¾
                content = content.replace(old_tag, new_tag)
                modified = True
        except Exception as e:
            log_message(f"ä¼˜åŒ–å›¾ç‰‡æ—¶å‡ºé”™: {img_path}, é”™è¯¯: {str(e)}")
    
    if modified:
        # å†™å›æ–‡ä»¶
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    
    return False

def enhance_mobile_seo(article_path):
    """å¢å¼ºç§»åŠ¨ç«¯SEOï¼Œæé«˜Core Web Vitalsåˆ†æ•°"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç§»åŠ¨ç«¯ä¼˜åŒ–
    if 'viewport' in content and 'mobile-optimization' in content:
        return False
    
    modified = False
    
    # 1. ç¡®ä¿æœ‰viewportå…ƒæ ‡ç­¾
    if 'viewport' not in content:
        head_end_pos = content.find('</head>')
        if head_end_pos != -1:
            viewport_meta = '<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0">\n    '
            content = content[:head_end_pos] + viewport_meta + content[head_end_pos:]
            modified = True
    
    # 2. æ·»åŠ ç§»åŠ¨ç«¯ä¼˜åŒ–CSS
    if 'mobile-optimization' not in content:
        style_section = '''
        <style class="mobile-optimization">
            /* ç§»åŠ¨ç«¯ä¼˜åŒ–æ ·å¼ */
            @media (max-width: 768px) {
                body {
                    font-size: 16px;
                    line-height: 1.6;
                }
                h1 {
                    font-size: 24px;
                    line-height: 1.3;
                }
                h2 {
                    font-size: 20px;
                }
                h3 {
                    font-size: 18px;
                }
                .container, .content {
                    padding-left: 15px;
                    padding-right: 15px;
                }
                img {
                    max-width: 100%;
                    height: auto;
                }
                /* æ”¹å–„è§¦æ‘¸ç›®æ ‡å°ºå¯¸ */
                a, button {
                    min-height: 44px;
                    min-width: 44px;
                }
                /* æ”¹å–„è¡¨å•å…ƒç´ åœ¨ç§»åŠ¨ç«¯çš„å¯ç”¨æ€§ */
                input, select, textarea {
                    font-size: 16px; /* é˜²æ­¢iOSç¼©æ”¾ */
                }
            }
        </style>
        '''
        
        head_end_pos = content.find('</head>')
        if head_end_pos != -1:
            content = content[:head_end_pos] + style_section + content[head_end_pos:]
            modified = True
    
    if modified:
        # å†™å›æ–‡ä»¶
        with open(article_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return True
    
    return False

def add_social_meta_tags(article_path):
    """æ·»åŠ ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾ï¼ˆOpen Graphå’ŒTwitter Cardï¼‰"""
    with open(article_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¤¾äº¤åª’ä½“æ ‡ç­¾
    if 'og:title' in content and 'twitter:card' in content:
        return False
    
    # è·å–æ–‡ç« æ ‡é¢˜
    title_match = re.search(r'<h1>(.*?)</h1>', content)
    if not title_match:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°æ–‡ç« æ ‡é¢˜")
        return False
    
    article_title = title_match.group(1)
    
    # è·å–æ–‡ç« æè¿°ï¼ˆä½¿ç”¨ç¬¬ä¸€æ®µè½ä½œä¸ºæè¿°ï¼‰
    description_match = re.search(r'<p>(.*?)</p>', content)
    article_description = ''
    if description_match:
        article_description = re.sub(r'<.*?>', '', description_match.group(1))
        if len(article_description) > 160:
            article_description = article_description[:157] + '...'
    
    # æŸ¥æ‰¾æ–‡ç« ä¸­çš„ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºç¤¾äº¤åª’ä½“å›¾ç‰‡
    img_match = re.search(r'<img\s+[^>]*src=["\']([^"\'>]+)["\'][^>]*>', content)
    article_image = ''
    if img_match:
        article_image = img_match.group(1)
        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹URLï¼ˆå‡è®¾ç½‘ç«™æ ¹ç›®å½•ï¼‰
        if not article_image.startswith('http'):
            article_image = f"/{article_image.lstrip('/')}" if article_image else ''
    
    # æ„å»ºç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾
    social_meta_tags = f'''
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:title" content="{article_title}">
    <meta property="og:description" content="{article_description}">
    <meta property="og:url" content="{urllib.parse.quote(os.path.basename(article_path))}">
    '''
    
    if article_image:
        social_meta_tags += f'''
    <meta property="og:image" content="{article_image}">
        '''
    
    social_meta_tags += f'''
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="{article_title}">
    <meta name="twitter:description" content="{article_description}">
    '''
    
    if article_image:
        social_meta_tags += f'''
    <meta name="twitter:image" content="{article_image}">
        '''
    
    # æŸ¥æ‰¾</head>æ ‡ç­¾ä½ç½®
    head_end_pos = content.find('</head>')
    if head_end_pos == -1:
        log_message(f"æ— æ³•åœ¨æ–‡ä»¶ {article_path} ä¸­æ‰¾åˆ°</head>æ ‡ç­¾")
        return False
    
    # æ’å…¥ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾
    new_content = content[:head_end_pos] + social_meta_tags + content[head_end_pos:]
    
    # å†™å›æ–‡ä»¶
    with open(article_path, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    return True

def update_articles():
    """æ›´æ–°éœ€è¦æ›´æ–°çš„æ–‡ç« """
    config = load_config()
    today = datetime.datetime.now().strftime('%Y-%m-%d')
    updated_count = 0
    
    # ç¡®ä¿å¤‡ä»½ç›®å½•å­˜åœ¨
    if not os.path.exists(BACKUP_DIR):
        os.makedirs(BACKUP_DIR)
    
    # ç¡®ä¿å›¾ç‰‡ç›®å½•å­˜åœ¨
    if not os.path.exists(IMAGES_DIR):
        os.makedirs(IMAGES_DIR)
    
    for article in config['articles']:
        if should_update_article(article):
            article_path = os.path.join(ARTICLES_DIR, article['file'])
            
            if not os.path.exists(article_path):
                log_message(f"æ–‡ä»¶ä¸å­˜åœ¨: {article_path}")
                continue
            
            # å¤‡ä»½æ–‡ç« 
            backup_path = backup_article(article_path)
            
            # æ›´æ–°æ–‡ç« æ—¥æœŸ
            date_updated = update_article_date(article_path)
            
            # æ ¹æ®æ–‡ç« ç±»å‹é€‰æ‹©æ›´æ–°ç­–ç•¥
            article_type = article.get('type', 'data')
            if article_type == 'core':
                # æ ¸å¿ƒå†…å®¹ï¼šåªæ·»åŠ æœ€æ–°æ›´æ–°åŒºå—ï¼Œä¸ä¿®æ”¹ä¸»ä½“
                content_updated = add_latest_update_section(article_path, article)
            else:
                # æ•°æ®å†…å®¹ï¼šæ·»åŠ æœ€æ–°æ›´æ–°åŒºå—å¹¶åœ¨æ­£æ–‡ä¸­æ’å…¥æ–°å†…å®¹
                update_section_added = add_latest_update_section(article_path, article)
                new_content_inserted = insert_new_content(article_path, article)
                content_updated = update_section_added or new_content_inserted
            
            # åº”ç”¨SEOä¼˜åŒ–
            log_message(f"å¼€å§‹å¯¹æ–‡ç«  {article['file']} åº”ç”¨SEOä¼˜åŒ–...")
            
            # 1. æ·»åŠ å†…éƒ¨é“¾æ¥ç»“æ„
            internal_links_added = add_internal_links(article_path, article, config['articles'])
            if internal_links_added:
                log_message(f"å·²æ·»åŠ å†…éƒ¨é“¾æ¥: {article['file']}")
            
            # 2. æ·»åŠ Schema.orgç»“æ„åŒ–æ•°æ®æ ‡è®°
            schema_added = add_schema_markup(article_path, article)
            if schema_added:
                log_message(f"å·²æ·»åŠ ç»“æ„åŒ–æ•°æ®æ ‡è®°: {article['file']}")
            
            # 3. ä¼˜åŒ–å›¾ç‰‡ï¼ˆæ·»åŠ altæ ‡ç­¾ã€å‹ç¼©å›¾ç‰‡ï¼‰
            images_optimized = optimize_images(article_path)
            if images_optimized:
                log_message(f"å·²ä¼˜åŒ–å›¾ç‰‡: {article['file']}")
            
            # 4. å¢å¼ºç§»åŠ¨ç«¯SEO
            mobile_enhanced = enhance_mobile_seo(article_path)
            if mobile_enhanced:
                log_message(f"å·²å¢å¼ºç§»åŠ¨ç«¯SEO: {article['file']}")
            
            # 5. æ·»åŠ ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾
            social_tags_added = add_social_meta_tags(article_path)
            if social_tags_added:
                log_message(f"å·²æ·»åŠ ç¤¾äº¤åª’ä½“å…ƒæ ‡ç­¾: {article['file']}")
            
            # æ›´æ–°æ–‡ç« çŠ¶æ€
            if date_updated or content_updated or internal_links_added or schema_added or \
               images_optimized or mobile_enhanced or social_tags_added:
                article['last_updated'] = today
                updated_count += 1
                log_message(f"å·²å®Œæˆæ–‡ç« æ›´æ–°å’ŒSEOä¼˜åŒ–: {article['file']} (ç±»å‹: {article_type})")
            else:
                log_message(f"æ›´æ–°å¤±è´¥: {article['file']}")
    
    # ä¿å­˜æ›´æ–°åçš„é…ç½®
    save_config(config)
    log_message(f"å®Œæˆæ›´æ–°ï¼Œå…±æ›´æ–° {updated_count} ç¯‡æ–‡ç« ")

if __name__ == "__main__":
    try:
        log_message("å¼€å§‹è‡ªåŠ¨æ›´æ–°æ–‡ç« ...")
        update_articles()
        
        # æ›´æ–°sitemap.xml
        log_message("å¼€å§‹æ›´æ–°sitemap.xml...")
        sitemap_updated = update_sitemap()
        if sitemap_updated:
            log_message("sitemap.xmlæ›´æ–°å®Œæˆ")
        else:
            log_message("sitemap.xmlæ›´æ–°å¤±è´¥")
        
        log_message("æ‰€æœ‰æ›´æ–°å®Œæˆ")
    except Exception as e:
        log_message(f"æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
